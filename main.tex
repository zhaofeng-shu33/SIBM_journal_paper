\documentclass[journal]{IEEEtran}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\1}{\mathbbm{1}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SIBM}{SIBM}
\DeclareMathOperator{\dist}{dist}

\title{Exact recovery of Stochastic Block Model by sampling from Ising model}
\author{
	Feng Zhao,~\IEEEmembership{Student Member, IEEE}\\
	Min Ye,~\IEEEmembership{Member, IEEE} and
	Shao-Lun~Huang,~\IEEEmembership{Member, IEEE}\\
	\thanks{Feng Zhao is with the
		Department of Electrical Engineering, Beijing, China.
		(Email: zhaof17@mails.tsinghua.edu.cn).
		Min Ye and S-L.~Huang are with the Data Science and Information
		Technology Research Center, Tsinghua-Berkeley Shenzhen Institute,
		Shenzhen, China (Email: \{yeemmi, shaolun.huang\}@sz.tsinghua.edu.cn).
	}}
	
\begin{document}
	\maketitle
\begin{abstract}
	Based on Ising model, we propose a stochastic algorithm to achieve the exact recovery for stochastic block model (SBM).
	The stochastic algorithm can be transformed to an optimization problem, which includes the special case of maximum likelihood.
	Besides, we give an unbiased convergent estimator for the parameters of SBM, which can be computed in constant time.
	Finally, we use metropolis sampling to realize the stochastic algorithm verify the error bound of our algorithm by experiments.
\end{abstract}
\begin{IEEEkeywords}
	stochastic block model, exact recovery, Ising model, modularity maximization, Metropolis sampling
\end{IEEEkeywords}
\section{Introduction}
Stochastic Block Model (SBM) is one of statistical modeling for community detection problems  \cite{holland1983stochastic, abbe2017community}.
It provides benchmark artificial dataset to evaluate different community detection algorithms.
Besides, SBM also inspires the design of algorithm for community detection tasks. These algorithms, such as
semi-definite relaxation, spectral clustering and label propagation, not only have theoretical guarantee when applied to SBM,
but perform well on dataset without SBM assumption. The study on the theoretical guarantee on SBM model can be divided between
the problem of exact recovery and that of partial recovery. For both cases, the asymptotic behavior of detection error
is analyzed when the scale of graph tends to infinity. There are already some well-known results of the exact recovery problem
on SBM.	To name but a few, Abbe and Mossel established the exact recovery region for a special sparse SBM with two communities  \cite{abbe2015exact, mossel2016}.
Later on, the result is extended to general SBM with multiple communities \cite{abbe2015community}.

Besides the recovery problem of SBM, inference of parameters on SBM provides a way to check whether the model fits the data well.
Previous inference methods require jointly estimation of node labels and model parameters \cite{nowicki2001estimation}, which have high complexity since the recovery and inference task are done simultaneously.
In this article, we will decouple the inference and recovery problem and propose an unbiased convergent estimator for SBM parameters when the number of communities is given. Once obtaining the estimator, the recovery condition can be checked to determine whether it is possible to recover the labels
exactly by other algorithms. Besides, the estimated parameter will guide the choice of parameters for our proposed stochastic algorithm.

Our analysis of maximal modularity is based on Ising model, which is a probability distribution of node states \cite{ising1925beitrag}.
Ising model is originally proposed in statistical mechanics to model the ferromagnetism phenomenon but has widely application in neuroscience, information theory
and social networks. Among different variants of Ising models, the phase transition property is shared. Based on the random graph generated by SBM with two underlining communities,
the Ising model is first studied by \cite{ye2020exact}. Our work will extend the existing result to multiple community case and establish the phase transition
property quantitatively by giving the recovery error upper bound. This error bound is polynomial decaying in two different phases respectively. Then we will propose a specialized Ising model using the definition of modularity. Sampling from this model,
we can achieve exact recovery for SBM.

Alternative to sampling from Ising model, we can find the state with maximal probability directly. This kind of method
is a generalization of maximum likelihood and also has connection with maximal modularity. Searching the state with maximal probability
could also be done within all balanced partition. We will show that the constrained search is equivalent with minimum cut problem and give the detection
error upper bound for both the unconstrained maximization and constrained one.

Exact solution to maximize the modularity or exact sampling from modularity-based Ising model is NP hard. Many algorithms have been proposed to find an approximation of maximal modularity in polynomial time.
Among these algorithms, simulated annealing performs well and produces a solution very close to the true maximal value \cite{liu2010detecting}.
On the other hand, in original Ising model,
metropolis sequential sampling is used to generate samples for Ising model \cite{metropolis1953equation}. Simulated annealing and metropolis sampling are closely related. In this article, we will
use metropolis sampling technique to sample from Ising model on SBM.

This paper is organized as follows. Firstly, in section \ref{sec:sibm} we introduce the stochastic Ising block model for both two and multiple communities, and illustrate its connection with maximum likelihood and modularity
maximization algorithm. Then in the next section \ref{sec:psbm}, we give a parameter estimator for SBM. Based on this estimator, in section \ref{sec:ms},
we propose a community detection method which use metropolis algorithm to generate sample for Ising model. Numerical experiments and conclusion are given at last
to finish this paper.

Throughout this paper, the number of community is denoted by $k$; $m$ is the number of samples; $\lfloor x \rfloor$ is the floor function of $x$; the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$;
the label of each node is a random variable $X_i$; $X_i$ is chosen from $W= \{1, \omega, \dots, \omega^{k-1}\}$ and we further require $W$
is a cyclic group with order $k$; $W^n$ is the n-ary Cartesian power of $W$;
$f$ is a permutation function on $W$ and applied to $W^n$ in elementwise way;
the set $S_k$ is used to represent all permutation functions on $W$ and $S_k(\sigma):=\{f(\sigma)| f\in S_k\}$ for $\sigma \in W^n$;
the indicator function $\delta(x,y)$ is defined as
$\delta(x,y) = 1 $ when $x=y$, and $\delta(x,y)=0$ when $x\neq y$;
$g(n) = \Theta(f(n))$ if there exists constant $c_1 < c_2$ such that $c_1 f(n) \leq g(n) \leq c_2 f(n)$
for large $n$;
$\Lambda := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k-1\}$
where $\mathbf{1}_n$ is the all one vector with dimension $n$;
we define the distance of two vectors as:
$\dist(\sigma, X)
=|\{i\in[n]:\sigma_i\neq X_i\}| \textrm{ for } \sigma,X\in W^n
$ and the distance of a vector to a space $S\subseteq W^n$
as
$\dist(\sigma,S)
:=\min\{\dist(\sigma, \sigma') | \sigma' \in S\}
$.
\section{Related works}
The classical Ising model is defined on lattice and confined to two state $\{\pm 1\}$. This definition
can be extended to general graph and multiple state \cite{potts1952some}. In \cite{liu2017log}, Liu considered
the Ising model defined on graph generated by sparse SBM and his focus is to compute the log partition function,
which is averaged over the random graphs. In \cite{berthet2019exact}, an Ising model with repulsive interaction
is considered on a fixed graph structure, and the phase transition condition is established which involves both the assortative and disassortative
parameters.
% our contribution and main results

\section{Stochastic Block Model and parameter estimation}\label{sec:psbm}
We consider a special symmetric stochastic block model, which is defined as follows:
	\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
	Let $0\leq q<p\leq 1$, $V=[n]$ and $X=(X_1,\dots,X_n)\in W^n$. $X$ satisfies the constraint that $|\{v \in [n] : X_v = u\}| = \frac{n}{k}$ for $u\in W$.
	The random graph $G$ is generated under $\SSBM(n,k,p,q)$ if
	\begin{enumerate}
	\item There is an edge of $G$ between the vertices $i$ and $j$ with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$
	\item The existence of each edge is independent with each other.
	\end{enumerate}
\end{definition}
To explain the generation of SSBM in more detail,
we use $Z_{ij}:=\mathbbm{1}[\{i,j\} \in E(G)]$, which is the indicator function of the existence of an edge between node $i$ and $j$.
Given the node labels $X$, $Z_{ij}$ is a Bernoulli random variable, whose expectation is given by:
\begin{equation}
\mathbb{E}[Z_{ij}] =
\begin{cases}
p & X_i = X_j \\ 
q & X_i \neq X_j
\end{cases}
\end{equation}
Then the random graph $G$ is completely specified by $Z:=\{Z_{ij}, 1\leq i<j\leq n\}$ in which all $Z_{ij}$ are jointly independent.
The probability distribution for SBM can be written as:
\begin{align}
&P_G(Z | X) = p^{\sum_{X_i = X_j} z_{ij}}q^{\sum_{X_i \neq X_j} z_{ij}} \notag \\
&\cdot \quad (1-p)^{\sum_{X_i = X_j} (1-z_{ij})}(1-q)^{\sum_{X_i \neq X_j} (1-z_{ij})} \label{eq:GmL}
\end{align}
We will use the notation $\cG_n$ to represent a set containing all graphs with $n$ nodes. By the normalization property,
$P_G(\cG_n) = \sum_{G\in \cG_n}P_G(G)=1$.

In Definition \ref{def:SSBM}, we have supposed that the node label $X$ is given instead of uniformly distributed random variable
in other literature. Since maximal posterior estimator is equivalent to maximum likehood when the prior is uniform,
these definition variants are equivalent, and our assumption on $X$ make the following analysis more concise.

Given the SBM, the exact recovery problem can be formally defined as follows:
\begin{definition}[Exact recovery in SBM] \label{def:SSBMR}
Given $X$, the random graph $G$ is drawn under $\SSBM(n,k,p,q)$. If there exists an algorithm that takes
$G$ as input and outputs $\hat{X}$ such that
\begin{equation*}
P(\hat{X} \in S_k(X)) \to 1 \textrm{ as } n \to \infty
\end{equation*}
\end{definition}

In the above definition, we have used the notation $\hat{X} \in S_k(X)$, which means that we can only
expect a recovery up to a global permutation of the ground truth label vector $X$. This is common in unsupervised
learning as no anchor exists to assign labels to different communities. Besides, given a graph $G$, the algorithm can
be either deterministic or stochastic. The probability of $\hat{X} \in S_k(X)$ should be understood as 
$\sum_{G \in \cG_n} P_G(G) P_{\hat{X}|G}(\hat{X} \in S_k(X))$, which reduced to 
$P_G(\hat{X} \in S_k(X))$ for deterministic algorithm.

For constant $p,q$, that is, $p,q$ is irrelevant with the graph size $n$,
we can always find algorithms to recover $X$ such that the detection error decreases exponentially with $n$.
That is to say, the task with dense graph is relatively easy to handle. Within the paper, we consider a case
when $p = \A, q = \B$. This case corresponds to the sparsest graph when exact recovery of SBM is possible.
And under this condition, a well known result \cite{abbe2015community} states that
exact recovery is possible if and only if
\begin{equation}\label{eq:abk}
\sqrt{a} - \sqrt{b} > \sqrt{k}
\end{equation}

We first consider the parameter estimation problem for SBM. Suppose $k$ is known and we want to estimate $a,b$ from the graph $G$.
Existing method needs to estimate the community structure and $a,b$ jointly. We give a simple method
by counting the number of edges $T_1$ and the number of triangles $T_2$ of $G$, and the estimators $\hat{a}, \hat{b}$ are
obtained by solving the following equation systems:
\begin{align}
\frac{x+(k-1)y}{2k} & = \frac{T_1}{n\log n} \label{eq:e_1}\\
\frac{1}{k^2}
\left(\frac{x^3}{6} + \frac{k-1}{2}xy^2 + (k-1)(k-2)\frac{y^3}{6}\right) & = \frac{T_2}{\log^3 n} \label{eq:e_2}
\end{align}
\begin{theorem}\label{thm:ab12}
When $n$ is large enough, the equation system \eqref{eq:e_1}, \eqref{eq:e_2} has unique solution $(\hat{a}, \hat{b})$,
which are unbiased consistent estimators
of $(a,b)$. That is,
$\mathbb{E}[\hat{a}] = a, \mathbb{E}[\hat{b}] = b$ and $\hat{a}, \hat{b}$ converges to $a,b$ respectively in probability.
\end{theorem}
Given a graph generated by SBM, we can use Theorem \ref{thm:ab12} to obtain the estimated $a,b$ and determine whether
exact recovery of label $X$ is possible by \eqref{eq:abk}.

Below we conduct a simple experiment to verify Theorem \ref{thm:ab12},
we consider several combinations of $(a,b,k)$ and obtain the estimator $(\hat{a}, \hat{b})$, using
the empirical square error $\frac{1}{m} \sum_{i=1}^m (\hat{a}-a)^2 + (\hat{b}-b)^2$ as the criterion,
choosing $m=1000$ we obtain the following figure.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.45\textwidth]{estimator-error-2020-11-12.eps}
	\caption{estimation error with respect to $n$}
\end{figure}
From this figure we can see that as $n$ increases, MSE decreases polynomially fast. Therefore, the convergence
of $\hat{a} \to a$ and $\hat{b} \to b$ can be verified from this experiment.

\section{Ising model for community detection}\label{sec:sibm}
Now we have defined SBM and its exact recovery problem, the definition of Ising model on a graph genered by SBM is given
as follows:
\begin{definition}[Ising model with $k$ states]\label{def:ising}
	The Ising model on a graph $G$ sampled from $\SSBM(n,k,\A,\B)$ with parameters $\gamma,\beta>0$
	is a probability distribution on the configurations $\sigma\in W^n$ such that
	\begin{align} \label{eq:isingma}
	P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{\exp(-\beta H(\bar{\sigma}))}{Z_G(\alpha,\beta)}
	\end{align}
	where
	\begin{equation}\label{eq:energy}
	H(\bar{\sigma}) = \gamma \frac{\log n}{n} \sum_{\{i,j\}\not\in E(G)} \delta(\bar{\sigma}_i, \bar{\sigma}_j)
	- \sum_{\{i,j\}\in E(G)} \delta(\bar{\sigma}_i, \bar{\sigma}_j)
	\end{equation}
	The subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and
	$Z_G(\alpha,\beta)$ is the normalizing constant for this distribution.
\end{definition}
In physics, we often call $\beta$ the inverse temperature and $Z_G(\gamma, \beta)$ the partition function.
The Hamiltonian energy $H(\bar{\sigma})$ consists of two terms: the repulsive interaction between nodes without edge connection
and the attractive interaction between nodes with edge connection. The term $\gamma$ is the ratio of the power of these two
interactions. We should add $\frac{\log n}{n}$ to balance the two terms because there are only $O(\frac{n}{\log n})$
connecting edges for each node.
The probability of each configuration is proportional to $\exp(-\beta H(\bar{\sigma}))$, and the configuration with largest
probability corresponds to the lowest energy.

There are two main difference of Definition \ref{def:ising} with the classical one. Firstly we add a compulsory term
between nodes without an edge connection. This makes these nodes have larger probability to take different labels.
Secondly, we allow the state at each node to take $k$ values from $W$.
When $\alpha = 0$ and $k=2$, Definition \ref{def:ising}
reduces to the classical definition up to a scaling factor.

The above definition of Ising model can be treated a probability distribution conditioned on $G$.
If we take an average over all graph sampled from SBM, we can get the marginal distribution for the state vector $\sigma$.
This is exactly what Stochastic Ising Block Model (SIBM) model says:
\begin{definition}[Stochastic Ising Block Model]
	Given a label $X$, $P_G$ is a distribution given by $\SSBM(n,k,\A,\B)$, Stochastic Ising Block Model
	is a probability distribution on $V$ such that
\begin{equation}\label{eq:sibm}
P_{\SIBM}(\sigma = \bar{\sigma}) = \sum_{G \in \cG_n} P_G(G) P_{\sigma | G}(\sigma = \bar{\sigma}) 
\end{equation}
\end{definition}
The SIBM, which is the marginal distribution of $(\bar{\sigma}, G)$, can be used directly as $\hat{X}$ for the exact recovery problem in Definition \ref{def:SSBMR}.  In such case, the property of the estimator $\hat{X}$ is determined
by the two parameters $(\gamma, \beta)$. When these parameters take proper value, the exact recovery of SBM is achievable, which is summarized in the following theorem:
\begin{theorem}\label{thm:phase_transition}
Define the function $g(\beta), \tilde{g}(\beta)$ as follows:
\begin{equation}
g(\beta) = \frac{be^{\beta} + a e^{-\beta}}{k} - \frac{a+b}{k} +1
\end{equation}
and
\begin{equation}
\tilde{g}(\beta) = \begin{cases}
g(\beta) & \beta \leq \bar{\beta} = \frac{1}{2}\log \frac{a}{b} \\
g(\bar{\beta}) = 1 - \frac{(\sqrt{a} - \sqrt{b})^2}{k} & \beta > \bar{\beta}
\end{cases}
\end{equation}
where $\bar{\beta} =  \arg\min_{\beta > 0} g(\beta)$.
Let $\beta^*$ be the solution to the equation $g(\beta) = 0$ and $\beta^* < \bar{\beta}$, then depending on
how $(\gamma, \beta)$ take values, $\forall \epsilon > 0$, when $n$ is sufficiently large, we have
\begin{enumerate}
\item If $\gamma > b$ and $\beta > \beta^*$,  $P_{\SIBM}(\sigma \not\in S_k(X)) \leq n^{\tilde{g}(\beta)/2 + \epsilon}$;
\item If $\gamma > b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma \in S_k(X)) \leq (1+o(1))\max\{n^{g(\bar{\beta})}, n^{-g(\beta) + \epsilon}\}$;
\end{enumerate}
\end{theorem}
By simple calculus, $\tilde{g}(\beta) < 0$ for $\beta> \beta^*$ and $g(\beta)>0$ for $\beta < \beta^*$.
For sufficiently small $\epsilon$ and as $n \to \infty$, the probability mentioned in Theorem \ref{thm:phase_transition} converges to $0$ at least
in polynomial speed.
Therefore, Theorem \ref{thm:phase_transition} establishes the sharp phase transition property of SIBM model.
To illustrate Theorem
\ref{thm:phase_transition} more clearly,
let $D(\sigma, \sigma')$ be the event when $\sigma$ is nearest to $\sigma'$ among all its permutation.
That is
\begin{equation}
D(\sigma, \sigma') := \{ \sigma = \arg\min_{f \in S_k} \dist(f(\sigma), \sigma')  \}
\end{equation}
Then Theorem 1 can be stated in the following way:
\begin{corollary}\label{cor:phase4}
\begin{enumerate}
	\item If $\gamma > b$ and $\beta > \beta^*$, $P_{\SIBM}(\sigma = X | D(\sigma, X))  = 1-o(1)$;
	\item If $\gamma > b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma = X | D(\sigma, X))  = o(1)$;
\end{enumerate}	
\end{corollary}

The rest of this section is devoted to the proof of Theorem \ref{thm:phase_transition}, which relies on the
careful analysis on the one-flip energy difference. This useful result is summarized in the following lemma:
\begin{lemma}\label{lem:lemmaDiff}
	Suppose $\bar{\sigma}'$ differs from $\bar{\sigma}$ only at position $r$ by $\bar{\sigma}'_r = \omega^s \cdot \bar{\sigma}_r$.
	Then the change of energy is
	\begin{align}
	H(\bar{\sigma}') - H(\bar{\sigma}) &= (1+\gamma \frac{\log n}{n})\sum_{i \in N_r(G)} J_s(\bar{\sigma}_r, \bar{\sigma}_i)
	\notag \\
	&- \gamma \frac{\log n}{n} (m(\bar{\sigma}_r)-m(\omega^s \cdot \bar{\sigma}_r)-1) \label{eq:DeltaH}
	\end{align}
	where $m(\omega^j) := |\{i \in [n] | \bar{\sigma}_i = \omega^j | \}$ and $J_s(x, y) = \delta(x, y) - \delta(\omega_s \cdot x, y)$
\end{lemma}
Lemma \ref{lem:lemmaDiff} gives an explicit way to compare the probability of two configurations by the following
equality:
\begin{equation}\label{eq:Pratio}
\frac{P_{\sigma |G } (\sigma = \bar{\sigma}')}{P_{\sigma |G } (\sigma = \bar{\sigma})}
= \exp(-\beta(H(\bar{\sigma}') - H(\bar{\sigma})))
\end{equation}
Besides, since the graph is sparse and every node has $O(\log n)$ neighbors, the computational cost for the energy difference
is also $O(\log n)$. 

When $H(\bar{\sigma}') < H(\bar{\sigma})$, we can expect $P_{\sigma | G}(\sigma = \bar{\sigma}')$ is far less than 
$P_{\sigma | G}(\sigma = \bar{\sigma})$. However,
since $G$ is sampled from SBM, the righthand side of \eqref{eq:Pratio} is a random variable. The stochastic behaviour
of $ \exp(-\beta(H(\bar{\sigma}') - H(\bar{\sigma}))) $ plays an important role when establishing Theorem \ref{thm:phase_transition}.
The case $\gamma > b$ has been analyzed in great detail in \cite{sibmmc}, here we enhance the existing result in the following lemma:
\begin{lemma}\label{lem:specialCase}
If $\gamma> b$ and $\beta > \beta^*$, there exists a set $\cG'_n$ such that
$P_G(\cG'_n) \geq 1 - (1+o(1))n^{\tilde{g}(\beta)/2}$ and for every $G\in \cG'_n$
\begin{equation}
\frac{P_{\sigma |G } (\sigma = \bar{\sigma}')}{P_{\sigma |G } (\sigma = X)} \leq n^{\tilde{g}(\beta)/2-1}
\end{equation}
where $\dist(\bar{\sigma}',X)=1$.
\end{lemma}
\begin{proof}
	From \eqref{eq:Pratio}, we only need to show that
	\begin{equation}\label{eq:beta_HP}
	P_G( \exp(-\beta(H(\bar{\sigma}') - H(\bar{\sigma}))) \geq n^{\tilde{g}(\beta)/2-1}) \leq (1+o(1))n^{\tilde{g}(\beta)/2}
	\end{equation}
	Taking $\bar{\sigma}=X$ in Lemma \ref{lem:lemmaDiff}, we have
	\begin{equation}\label{eq:energy_diff}
	H(\bar{\sigma}') - H(\bar{\sigma}) = (1+\gamma \frac{\log n}{n})(A^0_r - A^s_r) + \gamma\frac{\log n}{n}
	\end{equation}
	where $A_r^0 \sim Binom(\frac{n}{k}-1, \A)$ and $A^s_r \sim Binom(\frac{n}{k}, \B)$.
	Therefore, \eqref{eq:beta_HP} is equivalent with
	\begin{equation}\label{eq:pgAs}
	P_G\big(A^s_r - A_r^0 \geq t\log n\big)
	\leq (1+o(1))n^{\tilde{g}(\beta)/2}
	\end{equation}
	where $t=(1+o(1))\frac{1}{\beta}(\frac{\tilde{g}(\beta)}{2} -1)>\beta\frac{b-a}{k}+1$ and the $o(1)$ term is not random.
	\eqref{eq:pgAs} follows directly from Lemma \ref{lem:fb}.
\begin{lemma}[Lemma 1 of \cite{sibmmc}]\label{lem:fb}
	For $t\in [\frac{1}{k}(b-a), 0]$,
	define a function
	\begin{align}
	&f_{\beta}(t):=\frac{1}{k}\sqrt{k^2t^2+4ab} -\frac{a+b}{k} +1 +\beta t  \notag\\
	&-t\big(\log(\sqrt{k^2t^2+4ab}+kt)-\log(2b) \big). \label{eq:fbetat}
	\end{align}
	It follows that
	\begin{align} 
	& P_G(A^1_r-A^0_r\ge t\log n)  \notag\\
	\le &  \exp\Big(\log n \Big(f_{\beta}(t) -\beta t  - 1 + O\big(\frac{\log n}{n}\big) \Big)\Big) .
	\end{align}
	In particular, $P_G(A^1_r-A^0_r\ge 0 ) \leq n^{-\frac{(\sqrt{a}-\sqrt{b})^2}{k}} $.
\end{lemma}	
\end{proof}

Since 
$\frac{P_{\sigma |G } (\dist(\sigma, X)=1)}{P_{\sigma |G } (\sigma = X)} = \sum_{s=1}^{k-1}
\sum_{r=1}^n \frac{P_{\sigma |G } (\sigma = \bar{\sigma}'(s,r))}{P_{\sigma |G } (\sigma = X)}$ 
where $\bar{\sigma}'(s,r)$ only differs from $X$ by $\bar{\sigma}'_r = \omega^s \cdot X_r$.
In such a case, we can show that there exists a set $\cG'_n$
$P_G(\cG'_n) \geq 1 - (1+o(1))n^{\tilde{g}(\beta)/2}$ and 
$\frac{P_{\sigma |G } (\dist(\sigma, X)=1)}{P_{\sigma |G } (\sigma = X)} \leq n^{\tilde{g}(\beta)/2}$.
Or more generally, we have the following proposition
\begin{proposition}\label{prop:small}
If $\gamma>b$, $\beta>\beta^\ast$,
For $1\leq r \leq \frac{n}{\sqrt{\log n}}$
and $\forall \epsilon > 0$, there is a set $\cG^{(r)}$ such that
\begin{equation}\label{eq:Gr}
P_G(\cG^{(r)}_n) \ge 1 - n^{r(\tilde{g}(\beta)/2 + \epsilon)}
\end{equation}
and
for every $G\in\cG^{(r)}_n$,
\begin{equation}\label{eq:psigmaX}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r | D(\sigma, X))}
{P_{\sigma|G}(\sigma=X | D(\sigma, X))} <
n^{r \tilde{g}(\beta) /2}
\end{equation}
For $r> \frac{n}{\sqrt{\log n}}$, there is a set $\cG^{(r)}$ such that
\begin{equation}\label{eq:Gr1}
P(G\in\cG^{(r)}_n) \ge 1 - e^{-n}
\end{equation}
and
for every $G\in\cG^{(r)}_n$,
\begin{equation}\label{eq:psigmaX1}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r | D(\sigma, X))}
{P_{\sigma|G}(\sigma=X | D(\sigma, X))} <
e^{-n}
\end{equation}
\end{proposition}
We may tend to use Lemma \ref{lem:specialCase} to establish Proposition \ref{prop:small}. However, the set $\cG_n'$ may depend
on $(i,r)$ and it is not a good idea to use union bound to get the proper set. The rigorous proof relies on
proper choice of $\cG_n^{(r)}$
and is given in
the appendix.

Now we use Proposition \ref{prop:small} to show 1) of Theorem \ref{thm:phase_transition}.
\begin{proof}[Proof of Theorem \ref{thm:phase_transition}]
Since $P_{\SIBM}(\sigma \not \in S_k(X)) = \sum_{f\in S_k} P_{\SIBM}(\sigma \neq f(X) | D(\sigma, f(X))) P_{\SIBM}(D(\sigma, f(X)))$,
we only need to establish $P_{\SIBM}(\sigma \neq X | D(\sigma, X)) \leq k n^{\tilde{g}(\beta)/2 + \epsilon}$.
Let $\cG'_n = \cap_{r=1}^n \cG_n^{(r)}$, choose $\frac{\epsilon}{2}$, from \eqref{eq:Gr} we have
\begin{align*}
P_G(\cG'^c_n) &= P_G(\cup_{r=1}^n (\cG_n^{(r)})^c) \\
&\leq \sum_{r=1}^{n/\sqrt{\log n } } n^{r(\tilde{g}(\beta)/2 + \epsilon/2)}  + n e^{-n} \\
& \leq \frac{1}{2} n^{\tilde{g}(\beta)/2 + \epsilon}
\end{align*}
where the last equality follows from the estimation of sum of geometric series.
On the other hand, for every $G \in \cG'_n$, from \eqref{eq:psigmaX}
we have
\begin{align*}
\frac{P_{\sigma | G}(\sigma \neq X | D(\sigma, X))}{1-P_{\sigma | G}(\sigma \neq X | D(\sigma, X))} &= \frac{P_{\sigma | G}(\sigma \neq X | D(\sigma, X))}{P_{\sigma|G}(\sigma=X | D(\sigma, X))} \\
&< \sum_{r=1}^{n/\sqrt{\log n }}  n^{r\tilde{g}(\beta)/2} + n e^{-n}
\end{align*}
from which we can get the estimation $P_{\sigma | G}(\sigma \neq X | D(\sigma, X))\leq \frac{1}{2}n^{\tilde{g}(\beta)/2 + \epsilon}$.
Finally, 
\begin{align*}
P_{\SIBM}(\sigma \neq X|D(\sigma, X)) &= \sum_{G\in \cG'_n} P_G(G)P_{\sigma |G}(\sigma \neq X | D(\sigma, X)) \\
+ P_G(\cG'^c_n)
& \leq n^{\tilde{g}(\beta)/2 + \epsilon}
\end{align*}

\end{proof}
If $\gamma > b$ and $\beta < \beta^*$, we have the following proposition:
\begin{proposition}\label{prop:large2}
	If $\gamma > b$ and $\beta < \beta^*$, there is a set $\cG'_n$ such that
	\begin{equation}
	P_G(\cG'_n) \geq 1 - (1+o(1))\max\{n^{g(\bar{\beta})}, n^{\tilde{g}(2\beta_n) - 2g(\beta_n) + \epsilon} \}
	\end{equation}
	and for every $G \in \cG'_n$,
	\begin{equation}\label{eq:diff1g}
	\frac{P_{\sigma|G}(\dist(\sigma, X)=1 | D(\sigma, X))}
	{P_{\sigma|G}(\sigma=X | D(\sigma, X))} \geq (1+o(1))n^{g(\beta_n)}
	\end{equation}
	where $\beta_n = \beta(1+\gamma\frac{\log n}{n})$.
\end{proposition}
We will use Proposition \ref{prop:large2} to establish 2) of Theorem \ref{thm:phase_transition}.
\begin{proof}
	Using Proposition \ref{prop:large2}, for every $G \in \cG'_n$
	we can get
	$$
	\frac{1-P_{\sigma | G}(\sigma=X | D(\sigma, X))}{P_{\sigma | G}(\sigma=X | D(\sigma, X))}\geq (1+o(1))n^{g(\beta_n)}
	$$
	We then have
	$$
	P_{\sigma | G}(\sigma=X| D(\sigma, X)) \leq (1+o(1)) n^{-g(\beta_n)}
	$$
	Then
	\begin{align*}
	&P_{\SIBM}(\sigma=X| D(\sigma, X))  \leq  P(\cG'^c_n) \\
	&+ \sum_{G\in \cG'_n}P_G(G) P_{\sigma|G}(\sigma=X| D(\sigma, X)) \\
	& \leq (1+o(1))n^{g(\bar{\beta})} + (1+o(1)) \max\{n^{g(\bar{\beta})}, n^{\tilde{g}(2\beta_n) - 2g(\beta_n) + \epsilon} \}\\
	& \leq (1+o(1)) \max\{n^{g(\bar{\beta})}, n^{-g(\beta) + \epsilon}  \}
	\end{align*}
\end{proof}
\section{Community Detection via energy minimization}
Since $\beta^*$ is irrelevant with $n$, when $\gamma>b$, we can choose a sufficiently large $\beta$ such that
$\beta > \beta^*$, then by Theorem \ref{thm:phase_transition}, with almost 1 probability, $\sigma \in S_k(X)$, which
implies that $P_{\sigma | G}(\sigma = X)$ has largest probability for almost all graph $G$. Instead of generating
the sample from the Ising model, we can directly maximize the conditional probability to find the most favorable
configuration. Or equivalently, by minimizing the energy term in \eqref{eq:energy}:
\begin{equation}\label{eq:hatX}
\hat{X} := \arg\min_{\bar{\sigma} \in W^n} H(\bar{\sigma})
\end{equation}
When $\beta > \bar{\beta}$, $\tilde{g}(\beta)$ becomes a constant value, which is the fastest error rate upper bound we can obtain
from Theorem \ref{thm:phase_transition}. Therefore, for the estimator given by \eqref{eq:hatX}, we have the following
conclusion:
\begin{theorem}\label{thm:error_rate}
When $\sqrt{a} - \sqrt{b} > \sqrt{k}, \gamma > b$, for $n$ sufficiently large, $P_G(\hat{X} \not\in S_k(X)) \leq (k-1+o(1))n^{g(\bar{\beta})}$
\end{theorem}
Theorem \ref{thm:error_rate} is slightly stronger than \ref{thm:phase_transition} since the upper bound term
$n^{g(\bar{\beta})}$ decreases
faster than $n^{g(\bar{\beta})/2}$. This stronger result is obtained by considering the event when $P_G(G | X) < P_G(G | \sigma)$
for $\dist(\sigma, X) \geq 1$. Then using union bound to sum them up. This is also the proof technique to show exact recovery by maximum likelihood is possible
when $\sqrt{a} - \sqrt{b} > \sqrt{k}$, used in \cite{abbe2015exact} for $k=2$. For general case, since $\tilde{g}(\beta) = 1- \frac{(\sqrt{a} - \sqrt{b})^2}{k}$, Theorem \ref{thm:error_rate} implies that exact recovery is possible using $\hat{X}$ as long as  
$\sqrt{a} - \sqrt{b} > \sqrt{k}$ is satisfied.

In \eqref{eq:hatX}, we allow $\bar{\sigma}$ to take values from $W^n$. Since we know the ground truth has equal
size $|\{v \in [n] : X_v = u\}| = \frac{n}{k}$, another formulation is to restrict the search space to
$W^*:= \{\sigma \big\vert |\{v \in [n] : \sigma_v = \omega^s\}| = \frac{n}{k}, s=0,1, \dots, k \}$.
When $\sigma \in W^*$, minimizing $H(\sigma)$ is equivalent to
\begin{equation}
\hat{X}' = \min_{\sigma \in W^*} \sum_{\{i,j\} \not\in E(G) } \delta(\sigma_i, \sigma_j)
\end{equation}
which is the minimum cut between different detected communities.

When $\hat{X}' \neq X$, there must be errors of pairs instead of single one to satisfy the constraint $\hat{X}' \in W^*$.
Besides, the estimator of $\hat{X}'$ is parameter-free while $\hat{X}$ may depend on $\gamma$. The extra parameter $\gamma$ in the expression of
$\hat{X}$ can be regarded as a kind of Lagrange multiplier for this integer programming problem. Thus the optimization problem for $\hat{X}$
is a relaxation of that for $\hat{X}'$ by introducing a penalized term and enlarging the searched space from $W^*$ to $W$.

The following theorem shows that we can obtain a sharper upper bound for $\hat{X}'$ than that of $\hat{X}$:
\begin{theorem}\label{thm:error_rate_2}
	When $\sqrt{a} - \sqrt{b} > \sqrt{k}$, for $n$ sufficiently large, $P_G(\hat{X}' \not\in S_k(X)) \leq ((k-1)^2+o(1))n^{2g(\bar{\beta})}$
\end{theorem}
 
The energy has only one parameter $\gamma$, when it takes different values depending on $a,b$, minimizing the
energy is equivalent with some popular algorithms in asymptotic case. The following analysis shows
their relationship intuitively.

For maximum likelihood, from \eqref{eq:GmL} we can write
$$
\log P_G(Z|X=\sigma) = -\log\frac{a}{b} \cdot H(\sigma) + C
$$
where $\gamma \frac{\log n}{n} = \frac{1}{\log(a/b)}(\log (1-\A) - \log (1-\B))$ and $C$ is a constant irrelevant with $\sigma$.
When $n$ is sufficiently large, we have $\gamma \to \gamma_{ML} := \frac{a-b}{\log(a/b)}$.



For maximum modularity, we first consider the modularity of a graph, which is defined by
\begin{align}\label{eq:Q}
Q &= \frac{1}{2 |E|} \sum_{ij} (A_{ij} - \frac{d_i d_j}{2 |E|}) \delta(C_i, C_j)
\end{align}
where $d_i$ is the degree of the $i$-th node and $A$ is the adjacency matrix.
The modularity $Q$ can be re-written using the node label $\sigma$ as:
\begin{align}
Q(\sigma) = &-\sum_{\{i,j\} \not\in E(G) } \frac{d_i d_j}{2 |E|}\delta(\sigma_i,\sigma_j) \notag \\
&+ \sum_{\{i,j\} \in E(G) } (1 - \frac{d_i d_j}{2 |E|}) \delta(\sigma_i,\sigma_j)  \label{eq:Qtransform}
\end{align}
From \eqref{eq:Qtransform}, we can show that $Q(\sigma) \to -H(\sigma)$ with $\gamma = \gamma_{MQ} = \frac{a+b}{2}$ as $n\to \infty$.
Indeed, we have $d_i \sim \frac{\log n(a+b)}{2}, |E| \sim \frac{1}{2}n d_i$. Therefore, we have $\frac{d_id_j}{2|E|} \to \gamma_{MQ} \frac{\log n}{n} $.

Using $a>b$ and $\log x > 2 \frac{x-1}{x+1}$ for $x>1$ we can verify that $\gamma_{MQ} > \gamma_{ML} > b$. That is, both maximum likelihood and maximum modularity satisfy the exact recovery conditions.


\section{Community Detection based on Metropolis Sampling}\label{sec:ms}
From Theorem \ref{thm:phase_transition}, if we could sample from the Ising model, then with large probability, the sample
is aligned with $X$. However, the exact sampling is difficult when $n$ is very large since the state space increases in
a rate of $k^n$. Therefore, some approximation is
necessary. The most common way to generate an Ising sample is using Metropolis sampling \cite{metropolis1953equation}. 
Empirically speaking, starting from a random state, Metropolis algorithm updates the state by randomly selecting one position to flip at each iteration step.
Then after some initial burning time, the generated samples can be regarded samples from Ising model.

The theoretical guarantee of Metropolis sampling is based on Markov chain. Under some general conditions, Metropolis samples are converging to
the steady state of Markov chain. For Ising model, there is many previous works which have shown that
the convergence of Metropolis sampling \cite{diaconis1998we}.

For our specific Ising model and energy term in \eqref{eq:energy},
the pseudo code of our algorithm is summarized in \ref{alg:m}.
This algorithm requires the number of the communities $k$ to be known and the parameter $\gamma$ is given.
The iteration time $N$ should also be specified.
\begin{algorithm}[H]
	\caption{Metropolis sampling algorithm for SBM} \label{alg:m}
	Inputs: the graph $G$, inverse temperature $\beta$, disassortative power $\gamma$ \\
	Output: $\hat{X}$
	\begin{algorithmic}[1]
		\STATE random initialize $\sigma \in W^n$
		\FOR{$i=1,2,\dots, N$}
		\STATE propose a new state $\bar{\sigma}'$ according to Lemma \ref{lem:lemmaDiff} where $s, r$ are randomly chosen
		\STATE compute $\Delta H(r,s) = H(\bar{\sigma}') - H(\bar{\sigma})$ using \eqref{eq:DeltaH}
		\IF{$\Delta H(r,s)<0$}
		\STATE $\sigma_r \leftarrow w^s \cdot \sigma_r$
		\ELSE
		\STATE with probability $\exp(-\beta \Delta H(r,s))$
			such that $\sigma_r \leftarrow w^s \cdot \sigma_r$
		\ENDIF
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
The computation of $\Delta H(r,s)$ needs $O(\log n)$ time. Generally speaking, when there is disassortative interaction in
the Ising model, it needs to take $N=O(n\log n)$ to generate the sample for good approximation \cite{mcmc}.
Under such assumption, the time complexity of Algorithm \ref{alg:m} is $n \log^2 n$.

Using Metropolis sampling, we conduct a moderate simulation to verify Theorem \ref{thm:phase_transition}.
We choose $n=3200, k=2$ and generate the random graph $G$ 1000 times. For each $G$, we further sample $1000n$ states and compute its
occupation time on the state $\pm X$. The result is shown in the following figure.
\begin{figure}[!ht]
	\includegraphics[width=0.45\textwidth]{beta_trans-2020-11-13.eps}
	\caption{the accuracy of exact recovery by SIBM}
\end{figure}
The vertical red line $\beta=0.2$ represents the phase transition threshold. The point $(0.19,0.5)$ in the figure
can be regarded as the empirical phase transition threshold, whose first coordinate is near to $\beta^* = 0.2$.
The green line $(\beta, n^{g(\beta)/2})$ is the theoretical lower bound of accuracy for $\beta>0.2$ while the orange line
$(\beta, n^{-g(\beta)})$ is the theoretical upper bound of accuracy for $\beta < 0.2$. It can be expected that
as $n$ is larger, the empirical accuracy curve (blue line in the figure) will approach the step function, which jumps from
0 to 1 at $\beta=0.2$.
\section{Conclusion}
\appendix
\section*{Proof of Theorem \ref{thm:ab12}}
\begin{lemma}\label{lem:ER_tr_counting}
	Consider Erdos Renyi random graph $G$ with $n$ nodes, in which edges are placed independently with probability $p$. Suppose
	$p=\A$, the number of edges is denoted by $|E|$ while the number of triangles is $T$. Then
	$\frac{|E|}{n \log n} \to \frac{a}{2}$ and $\frac{T}{\log^3 n} \to \frac{a^3}{6}$ in probability.
\end{lemma}
\begin{proof}
		Let $X_{ij}$ represents a Bernoulli random variable with parameter $p$. Then $|E| = \sum_{i,j} X_{ij}$, $X_{ij}$ are i.i.d.
	$E[T(G)] = \frac{n(n-1)}{2}p = \frac{(n-1)\log n}{2}a$ and $\Var[|E|] = \frac{n(n-1)}{2} p(1-p) < a\frac{(n-1)\log n}{2}$
	Then by Chebyshev's inequality
	\begin{align*}
	P(\Big|\frac{|E|}{n \log n } - \frac{a}{2} \frac{n-1}{n}\Big| > \epsilon) & \leq
	\frac{\Var[|E| /(n \log n )]}{\epsilon^2} \\
	& < \frac{a(n-1)}{2n^2\epsilon^2\log n}
	\end{align*}
	For a given $\epsilon$, when $n$ is sufficiently large,
	\begin{align*}
	P(\Big|\frac{|E|}{n \log n } - \frac{a}{2} \Big| > \epsilon) & <
	P(\Big|\frac{|E|}{n \log n } - \frac{a}{2} \frac{n-1}{n}\Big| > 2\epsilon) \\
	& \leq \frac{n-1}{8n^2 \epsilon^2 \log n}
	\end{align*}
	Therefore, by the definition of convergence in probability, we have $\frac{|E|}{n \log n} \to \frac{a}{2}$ as $n\to \infty$.
	
	Let $X_{ijk}$ represents a Bernoulli random variable with parameter $p^3$.
	Then $T = \sum_{i,j,k} X_{ijk}$.
	It is easy to compute that $E[T] = \binom{n}{3}p^3$. Since $X_{ijk}$ are not independent, the variance of $T$ needs careful calculation.
	From \cite{holland1977method} we know that
	% Mordern version: https://stats.stackexchange.com/questions/338267/distribution-and-variance-of-count-of-triangles-in-random-graph
	\begin{align*}
	\Var[T] & = \binom{n}{3} p^3 + 12 \binom{n}{4} p^5 + 30 \binom{n}{5} p^6 + 20 \binom{n}{6} p^6 - \binom{n}{3}^2 p^6 \\
	 & = O(\log^3 n)
	\end{align*}
	Therefore
	by Chebyshev's inequality
	\begin{align*}
	P(\Big|\frac{T}{\log^3 n } - \frac{a^3}{6} \frac{(n-1)(n-2)}{n^2}\Big| > \epsilon) &\leq \frac{\Var[T /\log^3 n ]}{\epsilon^2} \\ 
	& = \frac{1}{\epsilon^2}O(\frac{1}{\log^3 n})
	\end{align*}
	Hence, $\frac{T}{\log^3 n} \to \frac{a^3}{6}$.
\end{proof}
The convergence of $|E|$ in ER graph can be extended directly to SBM since the existence of each edge is independent.
But for $T$, it is a little tricky since the existence of each triangle is dependent with each other.  We need the following two lemmas
to proceed.
\begin{lemma}\label{lem:SBM_tr_counting_cross}
	Consider a 2 community SBM $(2n, p, q)$ and count the number of triangles $T$ which has a node in $S_1$ and an edge in $S_2$.
Then the variance of $T$ is
\begin{align}
\Var[T] & = \frac{n^2(n-1)}{2}q^2p + n^2(n-1)(n-2)p^2q^3 \notag \\
& + \frac{n^2(n-1)^2}{2}q^4p - \frac{n^2(n-1)(3n-4)}{2}q^4 p^2 \label{eq:SBM_tr_counting_cross}
\end{align}
\end{lemma}
\begin{lemma}\label{lem:SBM_tr_counting_3}
	Consider a 3 community SBM$(3n, p, q)$ and count the number of triangles $T$ which has a node in $S_1$, one node in $S_2$ and one node in $S_3$.
	Then the variance of $T$ is
	\begin{equation*}\label{eq:SBM_tr_counting_three}
	\Var[T] = q^3 n^3 + 3q^4 n^3(n-1) + 3q^5 n^3 (n-1)^2 - n^3(3n^2-3n+1)q^6
	\end{equation*}
\end{lemma}
The proof of the above two lemmas uses some counting techniques similar to that in \cite{holland1977method}, and we omit it here.
\begin{lemma}\label{lem:sbmV}
	For a SBM$(n, k, p, q)$ where $p=\frac{a\log n}{n}, q = \frac{b\log n}{n}$. The number of triangles is $T$.
	Then $\frac{T}{(\log n)^3}$ converges to $\frac{1}{k^2}(\frac{a^3}{6} + \frac{k-1}{2}ab^2 + (k-1)(k-2)\frac{b^3}{6} )$ in probability as $n \to \infty$.
\end{lemma}
\begin{proof}
	We split $T$ into three parts, the first is the number of triangles within community $i$, $T_i$. There are $k$ terms of $T_i$.
	The second is the number of triangles which have one node in community $i$ and one edge in community $j$, $T_{ij}$. There are $k(k-1)$ terms of $T_{ij}$. The third is the number of triangles which have one node in community $i$, one node in community $j$ and one node in community $k$.
	
	We only need to show that
	\begin{align}
	\frac{T_i}{\log ^3 n} &\to \frac{(a/k)^3}{6} \\
	\frac{T_{ij}}{\log^3 n}& \to \frac{1}{2}(a/k)(b/k)^2\\
	\frac{T_{ijk}}{\log^3 n} & \to (b/k)^3
	\end{align}
	The convergence of $\frac{T_i}{\log ^3 n}$ comes from Lemma \ref{lem:ER_tr_counting}.
	For $T_{ij}$ we use the conclusion from Lemma \ref{lem:SBM_tr_counting_cross}.
	We replace $n$ with $n/k$, $p=a\frac{\log n}{n}$, $q=b\frac{\log n}{n}$ in Equation \eqref{eq:SBM_tr_counting_cross}.
	$\Var[T_{ij}] \sim \frac{ab^2}{2k^3} \log^3 n$. Since the expectation of $\frac{T_{ij}}{\log^3 n}$ is $(n/k)\binom{n/k}{2}pq^2/(\log^3 n)
	=\frac{n-1}{n}\frac{ab^3}{k^3}$. By Chebyshev's inequality we can show that 
	\begin{align*}
	P( \Big|\frac{T_{ij}}{\log^3 n} - \frac{n-1}{n}\frac{ab^3}{k^3} \Big| > \epsilon) &\leq \frac{\Var[T_{ij} / \log^3 n]}{\epsilon^2} \\
	& = \frac{1}{\epsilon^2}
	O(\frac{1}{\log^3 n})
	\end{align*}
	Therefore, $\frac{T_{ij}}{\log^3 n} $ converges to $\frac{1}{2}(a/k)(b/k)^2$
	
	To prove $\frac{T_{ijk}}{\log^3 n}\to (b/k)^3$, from Lemma \ref{lem:SBM_tr_counting_3} we can get $\Var[T_{ijk}] = O(\log^5 n)$
	$$
	P( \Big|\frac{T_{ijk}}{\log^3 n} -\frac{b^3}{k^3} \Big| > \epsilon) \leq \frac{\Var[T_{ijk} / \log^3 n]}{\epsilon^2} = \frac{1}{\epsilon^2}
	O(\frac{1}{\log n})
	$$
\end{proof}
\begin{proof}[Proof of Theorem \ref{thm:ab12}]
	Let $e^*_1 = \frac{a+(k-1)b}{2k}$, $e^*_2 = \frac{a^3}{6} + \frac{k-1}{2}ab^2 + (k-1)(k-2)\frac{b^3}{6}$
	and $e_1 = \frac{T_1}{n\log n}, e_2 = \frac{T_2}{\log^3 n}$.
	From Lemma \ref{lem:ER_tr_counting}, $e_1 \to e^*_1$.
	From Lemma \ref{lem:sbmV}, $e_2 \to e^*_2$ as $n\to \infty$.
	Using $x=2ke_1 - (k-1)y$ we can get
\begin{equation}
g(y): = (k-1)(y^3 - 6 e_1 y^2 + 12 e_1^2 y) + 6 e_2 - 8 k e_1^3 = 0
\end{equation}
This equation has unique real root since $g(y)$ is increasing on $\mathbb{R}$:  $g'(y) = 3(k-1)(y-2e_1)^2 \geq 0 $.
Next we show the root lies within $(0, 2e_1)$.
\begin{align*}
\lim_{n\to \infty}g(0) &=  6e^*_2 - 8k(e^*_1)^3 =-\frac{3}{k^2}(k-1)(k-2)ab^2 \\
 &- \frac{k-1}{k^2} ((k-2)-(k-1)^2)b^3 < 0 \\
\lim_{n\to \infty}g(2e_1) &= 6e^*_2 - 8(e^*_1)^3 = \frac{(k-1)(a-b)^3}{k^3} > 0
\end{align*}

Therefore, we can get a unique solution $y$ within $(0, 2e_1)$. Since $(a,b)$ is a solution for the equation array. The conclusion follows.

By taking expectation on both sizes of \eqref{eq:e_1}, \eqref{eq:e_1} we can show $\mathbb{E}[\hat{a}] = a,
\mathbb{E}[\hat{b}] = b$. By the continuous property of $g(y)$, $\hat{b} \to b$ and $\hat{a} \to a$ follows similarly.
\end{proof}
\section*{Proof of Proposition \ref{prop:small}}
We distinguish the discussion between two cases: $r\leq \frac{n}{\sqrt{\log n}}$
and $r > \frac{n}{\sqrt{\log n}}$. Some additional notation is needed for the proof.
Similar with \eqref{eq:energy_diff}, generally we can write
$$
H(\bar{\sigma}) - H(X)=
(1 + \gamma \frac{ \log n}{n})[A_{\bar{\sigma}} - B_{\bar{\sigma}}] + \gamma\frac{ \log n}{n} N_{\bar{\sigma}}
$$
in which we use $A_{\bar{\sigma}}$ (or $B_{\bar{\sigma}}$) to represent the binomial random variable with parameter $\A$ (or $\B$)
respectively and $N_{\bar{\sigma}}$ is a deterministic positive number, irrelevant with the graph structure.
\begin{lemma}[Lemma 5 from \cite{sibmmc}]\label{lem:sigmaX}
	Let $\gamma > b$. When $\dist(\bar{\sigma}, X) \geq \frac{n}{\sqrt{\log n}}$ and $\arg\,\min_{\sigma'\in S_k(X)} \dist(\bar{\sigma}, \sigma') = X$. Show that
	$P_{\sigma | G}(\sigma = \bar{\sigma} ) > \exp(-Cn) P_{\sigma | G}(\sigma = X)$
	happens with probability (w.r.t. SSBM) less than $\exp(-\tau(\alpha,\beta) n \log^{1-\delta} n )$ where $C$ is an arbitrary constant, $\tau(\alpha,\beta)$ is a positive number.
\end{lemma}
When $r\leq \frac{n}{\sqrt{\log n}}$, we can show that $\dist(\sigma, X) = r$ implies $D(\sigma, X)$ by using the triangle
inequality of $\dist$. For $f \in S_k \backslash \{ id \}$, we have
$$
\frac{2n}{k} \leq \dist(f(X), X) \leq \dist(\sigma, f(X)) + \dist(\sigma, X)
$$
Therefore, $\dist(\sigma, f(X)) \geq \frac{2n}{k} - \frac{n}{\sqrt{\log n}} \geq \dist(\sigma, X)$ and
\eqref{eq:psigmaX} is equivalent with
\begin{equation}\label{eq:psigmaX2}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r)}
{P_{\sigma|G}(\sigma=X)} <
n^{r \tilde{g}(\beta) /2}
\end{equation}
The left hand side can be written as:
\begin{align*}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r)}
{P_{\sigma|G}(\sigma=X)}  &= \sum_{\dist(\bar{\sigma}, X)=r}\exp(-\beta(H(\bar{\sigma})-H(X)))\\
&\leq \sum_{\dist(\bar{\sigma}, X)=r}\exp(\beta_n(B_{\bar{\sigma}}-A_{\bar{\sigma}}))
\end{align*}


Define $\Xi_n(r): = \sum_{\dist(\bar{\sigma}, X)=r}\exp(\beta_n(B_{\bar{\sigma}}-A_{\bar{\sigma}}))$ and we only need to show that
\begin{equation}
P_{G}(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2}) \leq  n^{r (\tilde{g}(\beta) /2 + \epsilon)}
\end{equation}
Define the event $\Lambda_n(G,r):=\{B_{\bar{\sigma}} -A_{\bar{\sigma}} < 0, \forall \dist(\bar{\sigma}, X)=r\}$,
we proceed as follows:
\begin{align*}
P_{G}(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2}) &\leq
P_G(\Lambda_n(G,r)^c) \\
&+ P_G(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2} |\Lambda_n(G,r) )
\end{align*}
For the first term, 
$P_G(\Lambda_n(G,r)^c) \leq n^{rg(\bar{\beta})} \leq n^{r (\tilde{g}(\beta) /2 + \epsilon/2)}$.
For the second term, we use Markov inequality:
\begin{align*}
P_G(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2} |\Lambda_n(G,r) )
\leq \mathbb{E}[\Xi_n(r)|\Lambda_n(G,r)]n^{-r \tilde{g}(\beta) /2} 
\end{align*}
The conditional expectation can be estimated as follows:
\begin{align*}
&\mathbb{E}[\Xi_n(r)|\Lambda_n(G,r)]=
\sum_{\dist(\bar{\sigma}, X) = r}\sum_{tr\log n = -\infty }^{-1} \\
& P_G(B_{\bar{\sigma}} -A_{\bar{\sigma}}=tr\log n)\exp(\beta_n tr \log n) \\
& \leq (k-1)^r n^{r+r\beta(b-a)/k} +
\sum_{\dist(\bar{\sigma}, X) = r}\sum_{tr\log n = r(b-a)/k\log n }^{-1} \\
& P_G(B_{\bar{\sigma}} -A_{\bar{\sigma}}=t\log n)\exp(\beta_n rt \log n)
\end{align*}
$r+r\beta(b-a)/k = f_{\beta}(\frac{b-a}{k}) < \tilde{g}(\beta)$, therefore,
$(k-1)^r n^{r+r\beta(b-a)/k}n^{-r \tilde{g}(\beta) /2} \leq n^{r (\tilde{g}(\beta) /2 + \epsilon/2)} $.
Using Lemma \ref{lem:enhanced_fb}, we have
\begin{align*}
P_G(B_{\bar{\sigma}} -A_{\bar{\sigma}}=t\log n)\exp(\beta_n rt \log n) \leq 
n^{r(f_{\beta_n}(t)-1 + O(\frac{1}{\sqrt{\log n}}))}
\end{align*}
Since $\beta_n \to \beta$, $\forall \epsilon$, when $n$ is sufficiently large
we have $\tilde{g}(\beta_n) \leq \tilde{g}(\beta) + \epsilon /2$.
Therefore,
\begin{align*}
&\sum_{\dist(\bar{\sigma}, X) = r}\sum_{\substack{tr\log n = \\ r(b-a)/k\log n} }^{-1}
P_G(B_{\bar{\sigma}} -A_{\bar{\sigma}}=t\log n)\exp(\beta_n rt \log n) \\
& \leq  n^{r(\tilde{g}(\beta_n) - \tilde{g}(\beta)/2)}\\
& \leq  n^{r(\tilde{g}(\beta)/2 + \epsilon/2)} O(\log n) (k-1)^r
\end{align*}
Combining the above equations, we have
\begin{align*}
P_{G}(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2}) &\leq  n^{r(\tilde{g}(\beta)/2 + \epsilon/2)} O(\log n) (k-1)^r\\
&\leq n^{r(\tilde{g}(\beta)/2 + \epsilon)}
\end{align*}
When $r>\frac{n}{\sqrt{\log n}}$, using Lemma \ref{lem:sigmaX}, we can choose a sufficiently large constant $C>1$
such that $k^n\exp(-Cn) < e^{-n}$
\begin{align*}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r | D(\sigma, X))}
{P_{\sigma|G}(\sigma=X | D(\sigma, X))} &= \sum_{\substack{D(\sigma, X) \\ \dist(\sigma, X)=r}} \frac{P_{\sigma | G}(\sigma = \bar{\sigma}) }{P_{\sigma | X}(\sigma = X)} \\
&> \exp(-Cn)
\end{align*}
happens with probability less than $e^{-n}$. Therefore, \eqref{eq:psigmaX1} holds.
\begin{lemma}[Lemma 6 in \cite{sibmmc}]\label{lem:enhanced_fb}
	For $t\in [\frac{1}{k}(b-a), 0]$
	and $ r \le n/\sqrt{\log n}$
	\begin{equation} \label{eq:upmpt}
	\begin{aligned}
	& P_G(B_{\bar{\sigma}}-A_{\bar{\sigma}}\ge t r \log n)  \\
	\le & \exp\Big(r\log n
	\Big(f_{\beta}(t) - \beta t -1	+ O(\frac{1}{\sqrt{\log n}}) \Big)\Big) .
	\end{aligned}
	\end{equation}
\end{lemma}
\section*{Proof of Proposition \ref{prop:large2}}
The left hand of \eqref{eq:diff1g} can be rewritten as:
\begin{equation}\label{eq:knd}
	\frac{P_{\sigma|G}(\dist(\sigma, X)=1)}
{P_{\sigma|G}(\sigma=X)}= (1+o(1))\sum_{s=1}^{k-1}\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0))
\end{equation}
From Proposition 7, there is a set $\cG^{(1)}_n$ such that $P_G(\cG^{(1)}_n) \geq 1-n^{g(\bar{\beta})}$
and
\begin{align}
\mathbb{E}[\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0)) | G \in \cG^{(1)}_n] &= (1+o(1))n^{g(\beta_n)} \\
\Var[\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0)) | G \in \cG^{(1)}_n] &\leq n^{\tilde{g}(2\beta_n)} \notag \\
&+ C_n n^{2g(\beta_n)-1}\log n
\end{align}
where $C_n = be^{2\beta_n}(e^{\beta_n}-1)^2$.

Since $\tilde{g}(\beta)$ is convex,
we have $2\tilde{g}(x) < \tilde{g}(0) + \tilde{g}(2x)$. That is $\tilde{g}(2\beta_n) - g(\beta_n) > g(\beta_n) - 1$.
Therefore, the variance term is actually bounded by $n^{\tilde{g}(2\beta_n)} $.

Let $\cG^{(2)}_n: = \{|\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0)) - (1+o(1))n^{g(\beta_n)}  | \leq n^{g(\beta_n) - \epsilon / 2} \}$,

Using Chebyshev inequality, we have
\begin{equation*}
P_G(G \not\in \cG^{(2)}_n \Big\vert  G \in \cG^{(1)}_n) \leq n^{\tilde{g}(2\beta_n) - 2g(\beta_n) + \epsilon}
\end{equation*}
Let $\cG'_n = \cG^{(1)}_n \cap \cG^{(2)}_n$
\begin{align*}
P_G(G \in \cG'_n) &= P_G(\cG^{(1)}_n) P_G(G \in \cG_n^{(2)} | G \in \cG_n^{(1)}) \\
& \geq (1-n^{\tilde{g}(2\beta_n) - 2g(\beta_n) + \epsilon})(1-n^{g(\bar{\beta})}) \\
&= 1-(1+o(1))\max\{n^{g(\bar{\beta})}, n^{\tilde{g}(2\beta_n) - 2g(\beta_n) + \epsilon} \}
\end{align*}
and for every $G\in\cG'_n$,
\begin{equation*}
\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0)) = (1+o(1)) n^{g(\beta_n)}
\end{equation*}
Therefore, from \eqref{eq:knd} we have
\begin{equation*}
	\frac{P_{\sigma|G}(\dist(\sigma, X)=1)}
{P_{\sigma|G}(\sigma=X)} \geq (1+o(1)) n^{g(\beta_n)}
\end{equation*}
\section*{Proof of Theorem \ref{thm:error_rate} and Theorem \ref{thm:error_rate_2}}
Let $P_F^{(r)}$ denote the probability when there is $\sigma$ satisfying $\dist(\sigma, X) = r$ and $H(\sigma) < H(X)$.

From \eqref{eq:energy_diff}, when $\sigma$ differs from $X$ only by one coordinate, the probability for $H(\sigma) < H(X)$ is
bounded by $P_G(A_r^s - A_r^0 > 0) = n^{-\frac{(\sqrt{a}-\sqrt{b})^2}{k}}$. Therefore, $P_F^{(1)}  \leq (k-1)n^{g(\bar{\beta})}$.
Using Lemma \ref{lem:enhanced_fb}, we can get $P_F^{(r)} \leq (k-1)^r n^{rg(\bar{\beta})}$ for $ r \leq \frac{n}{\sqrt{\log n}}$.
For $ r \geq \frac{n}{\sqrt{\log n}}$, choosing $C=0$ in Lemma \ref{lem:sigmaX} we can get $\sum_{r\geq \frac{n}{\sqrt{\log n}}}P_F^{(r)} \leq e^{-n}$.
That is, the dominant term is $\sum_{r\leq \frac{n}{\sqrt{\log n}}}P_F^{(r)}$ since the other part decreases exponentially fast.
Therefore, the upper bound for error rate is
\begin{align*}
P_F = & \sum_{r=1}^n P_F^{(r)} \leq (1+o(1)) \sum_{r=1}^{\infty} (k-1)^r n^{rg(\bar{\beta})}\\
 \leq & (1+o(1))\frac{(k-1) n^{g(\bar{\beta})}}{1-(k-1) n^{g(\bar{\beta})}} = (k-1+o(1))n^{g(\bar{\beta})}
\end{align*}
%\begin{the
\begin{lemma}\label{lem:mZW}
	Let $m$ be a positive number larger than $n$.
	When $Z_1, \dots, Z_m$ are i.i.d. Bernoulli($\B$) and $W_1, \dots, W_m$ are i.i.d Bernoulli($\A$), independent of $Z_1, \dots, Z_m$,
	then
	\begin{equation}
	P(\sum_{i=1}^m (Z_i  - W_i) \geq 0) \leq \exp(-\frac{m \log n}{n}(\sqrt{a} - \sqrt{b})^2)
	\end{equation}
\end{lemma}
\begin{proof}[Proof of Theorem \ref{thm:error_rate_2}]
When $\sigma \in W^*$, since $|\{r\in [n] | \sigma_r = \omega^i \}| = I'_i + \frac{n}{k} - I_i $, we have $I'_i = I_i$.
From the expression of $|B_{\bar{\sigma}}|$ and $|A_{\bar{\sigma}}|$, we can see  $|B_{\bar{\sigma}}| = |A_{\bar{\sigma}}|$
and $N_{\bar{\sigma}} = 0$. $H(\bar{\sigma}) < H(X)$ is equivalent with $B_{\bar{\sigma}} > A_{\bar{\sigma}}$.

Therefore, when $ \dist(\bar{\sigma}, X) \geq \frac{n}{\sqrt{\log n} }$ and $D(\sigma, X)$,
we must have $A_{\bar{\sigma}} \geq \frac{n^2}{k^2(k-1)\sqrt{\log n} } (1+o(1))$.
We use Lemma \ref{lem:mZW} by letting $m=|A_{\bar{\sigma}}|$ when $m \geq \frac{n}{ \sqrt{\log n}}$, the error term is bounded
by $\sum_{\frac{n}{ \sqrt{\log n}}} P_F^{(r)} \leq \sum_{\frac{n}{ \sqrt{\log n}}} (k-1)^r \exp(-\frac{(\sqrt{a} - \sqrt{b})^2}{k^2(k-1)} n \sqrt{\log n}))
\leq \exp(-n)$, which decreases exponentially fast.
For $m < \frac{n}{ \sqrt{\log n}}$, we can use Lemma \ref{lem:enhanced_fb} directly 
by considering $\sum_{r=2}^{\frac{n}{ \sqrt{\log n}}} P_F^{(r)}$. The summation starts from $r=2$ since $\sigma \in W^*$.
Therefore,
\begin{align*}
P_F = & \sum_{r=2}^n P_F^{(r)} \leq (1+o(1)) \sum_{r=2}^{\infty} (k-1)^r n^{rg(\bar{\beta})}\\
\leq & ((k-1)^2+o(1))n^{2g(\bar{\beta})}
\end{align*}
\end{proof}
\bibliographystyle{IEEEtran}
\bibliography{exportlist}

\end{document}