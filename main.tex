\documentclass[journal]{IEEEtran}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\newcommand{\cG}{\mathcal{G}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SIBM}{SIBM}
\DeclareMathOperator{\dist}{dist}

\title{Exact recovery of Stochastic Block Model with Ising sampling}
\author{
	Feng Zhao,~\IEEEmembership{Student Member, IEEE}\\
	Min Ye,~\IEEEmembership{Member, IEEE} and
	Shao-Lun~Huang,~\IEEEmembership{Member, IEEE}\\
	\thanks{Feng Zhao is with the
		Department of Electrical Engineering, Beijing, China.
		(Email: zhaof17@mails.tsinghua.edu.cn).
		Min Ye and S-L.~Huang are with the Data Science and Information
		Technology Research Center, Tsinghua-Berkeley Shenzhen Institute,
		Shenzhen, China (Email: \{yeemmi, shaolun.huang\}@sz.tsinghua.edu.cn).
	}}
	
\begin{document}
	\maketitle
\begin{abstract}
	Based on Ising sampling, we propose a stochastic algorithm to achieve the exact recovery for stochastic block model (SBM).
	The stochastic algorithm can be transformed to an optimization problem, which includes maximum likelihood and maximal modularity.
	Besides, we give an unbiased convergent estimator of the parameters of SBM, which can be computed in constant time.
	Finally, we use metropolis sampling to realize the theoretical Ising sampling and demonstrates the better performance of our method,
	compared with other theoretically guaranteed algorithm.
\end{abstract}
\begin{IEEEkeywords}
	stochastic block model, exact recovery, Ising model, modularity maximization, Metropolis sampling
\end{IEEEkeywords}
\section{Introduction}
Stochastic Block Model (SBM) is one of statistical modeling for community detection problems  \cite{holland1983stochastic, abbe2017community}.
It provides benchmark artificial dataset to evaluate different community detection algorithms.
Besides, SBM also inspires the design of algorithm for community detection tasks. These algorithms, such as
semi-definite relaxation, spectral clustering and label propagation, not only have theoretical guarantee when applied to SBM,
but perform well on dataset without SBM assumption. The study on the theoretical guarantee on SBM model can be divided between
exact recovery and partial recovery. For both cases, the asymptotic behavior of detection error
is analyzed when the scale of graph tends to infinity. There are already some well-known results of the exact recovery problem
on SBM.	To name but a few, Abbe and Mossel established the exact recovery region for a special sparse SBM with two communities  \cite{abbe2015exact, mossel2016}.
Later on, the result is extended to general SBM with multiple communities \cite{abbe2015community}.

Besides theoretical study on SBM, in community detection maximal modularity is a popular detection method \cite{Newman8577}.
The modularity is a kind of criteria and object function. Though maximal modularity works well in many practical problems, it's
generally unknown whether it can achieve exact recovery for SBM model. Newman, the inventor of modularity, demonstrates that
the modularity method is equivalent with maximum likelihood for a degree-corrected SBM model \cite{newman2016equivalence}. In this article,
we will fill the gap to prove that maximal modularity can achieve exact recovery for symmetric SBM model.

Our analysis of maximal modularity is based on Ising model, which is a probability distribution of node states \cite{ising1925beitrag}.
Ising model is originally proposed in statistical mechanics to model the ferromagnetism phenomenon but has widely application in neuroscience, information theory
and social networks. Among different variants of Ising models, the phase transition property is shared. Based on the random graph generated by SBM with two underlining communities,
the Ising model is first studied by \cite{ye2020exact}. Our work will extend the existing result to multiple community case and establish the phase transition
property and sample complexity results. Then we will propose a specialized Ising model using the definition of modularity. Sampling from this model,
we can also achieve exact recovery for SBM.

In order to achieve exact recovery of SBM, the extra parameters of Ising model should be carefully chosen, which depends on the parameters of SBM. 
Previous methods require jointly estimation of node labels and model parameters \cite{nowicki2001estimation}, which are not suitable when only model parameters are required.
In this article, we propose an unbiased convergent estimator for SBM parameters when the number of communities is given. This estimator is not only useful
for Ising sampling, but also beneficial for other recovery algorithms which requires SBM parameters.

Exact solution to maximize the modularity or exact sampling from modularity-based Ising model is NP hard. Many algorithms have been proposed to find an approximation of maximal modularity in polynomial time.
Among these algorithms, simulated annealing performs well and produces a solution very close to the true maximal value \cite{liu2010detecting}.
On the other hand, in original Ising model,
metropolis sequential sampling is used to generate samples for Ising model \cite{metropolis1953equation}. Simulated annealing and metropolis sampling are closely related. In this article, we will
use metropolis sampling technique to sample from Ising model on SBM. We will demonstrate by experiments that for non-exact recovery region of SBM our method outperforms other
theoretically guaranteed algorithm like SDP and spectral clustering.

This paper is organized as follows. Firstly, in section \ref{sec:sibm} we introduce the stochastic Ising block model for both two and multiple communities.
Then in the next section \ref{sec:dcsibm}, we propose Degree-corrected Stochastic Ising Block Model and uses this model to prove the exact recovery by modularity
maximization algorithm. Besides, in section \ref{sec:psbm}, we give a parameter estimator for SBM. Based on this estimator, in section \ref{sec:ms},
we propose a community detection method which use metropolis algorithm to generate sample for Ising model. Numerical experiments and conclusion are given at last
to finish this paper.

Throughout this paper, the number of community is denoted by $k$; $m$ is the number of samples; $\lfloor x \rfloor$ is the floor function of $x$; the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$;
the label of each node is a random variable $X_i$; $X_i$ is chosen from $W= \{1, \omega, \dots, \omega^{k-1}\}$ and we further require $W$
is a cyclic group with order $k$; $W^n$ is the n-ary Cartesian power of $W$;
$f$ is a permutation function on $W$ and applied to $W^n$ in elementwise way;
the set $S_k$ is used to represent all permutation functions on $W$ and $S_k(\sigma):=\{f(\sigma)| f\in S_k\}$ for $\sigma \in W^n$;
the indicator function $\delta(x,y)$ is defined as
$\delta(x,y) = 1 $ when $x=y$, and $\delta(x,y)=0$ when $x\neq y$;
$g(n) = \Theta(f(n))$ if there exists constant $c_1 < c_2$ such that $c_1 f(n) \leq g(n) \leq c_2 f(n)$
for large $n$;
$\Lambda := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k-1\}$
where $\mathbf{1}_n$ is the all one vector with dimension $n$;
we define the distance of two vectors as:
$\dist(\sigma, X)
=|\{i\in[n]:\sigma_i\neq X_i\}| \textrm{ for } \sigma,X\in W^n
$ and the distance of a vector to a space $S\subseteq W^n$
as
$\dist(\sigma,S)
:=\min\{\dist(\sigma, \sigma') | \sigma' \in S\}
$.
\section{Related works}
The classical Ising model is defined on lattice and confined to two state $\{\pm 1\}$. This definition
can be extended to general graph and multiple state \cite{potts1952some}. In \cite{liu2017log}, Liu considered
the Ising model defined on graph generated by sparse SBM and his focus is to compute the log partition function,
which is averaged over the random graphs. In \cite{berthet2019exact}, an Ising model with repulsive interaction
is considered on a fixed graph structure, and the phase transition involving both the assortative and disassortative
parameters.
% our contribution and main results

\section{Stochastic Ising Block Model}\label{sec:sibm}
We consider a special symmetric stochastic block model, which is defined as follows:
	\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
	Let $0\leq q<p\leq 1$, $V=[n]$ and $X=(X_1,\dots,X_n)\in W^n$. $X$ satisfies the constraint that $|\{v \in [n] : X_v = u\}| = \frac{n}{k}$ for $u\in W$.
	The random graph $G$ is generated under $\SSBM(n,k,p,q)$ if
	\begin{enumerate}
	\item There is an edge of $G$ between the vertices $i$ and $j$ with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$
	\item The existence of each edge is independent with each other.
	\end{enumerate}
\end{definition}
To explain the generation of SSBM in more detail,
we use $Z_{ij}:=\mathbbm{1}[\{i,j\} \in E(G)]$, which is the indicator function of the existence of an edge between node $i$ and $j$.
Given the node labels $X$, $Z_{ij}$ is a Bernoulli random variable, whose expectation is given by:
\begin{equation}
\mathbb{E}[Z_{ij}] =
\begin{cases}
p & X_i = X_j \\ 
q & X_i \neq X_j
\end{cases}
\end{equation}
Then the random graph $G$ is completely specified by $\{Z_{ij}, 1\leq i<j\leq n\}$ in which all $Z_{ij}$ are jointly independent.
The probability distribution for SBM can be written as:
\begin{align}
&P_G(Z_{ij} = z_{ij}, 1\leq i<j\leq n) = p^{\sum_{X_i = X_j} z_{ij}}q^{\sum_{X_i \neq X_j} z_{ij}} \cdot \notag \\
&\quad (1-p)^{\sum_{X_i = X_j} (1-z_{ij})}(1-q)^{\sum_{X_i \neq X_j} (1-z_{ij})}
\end{align}
We will use the notation $\cG_n$ to represent a set containing all graphs with $n$ nodes. By the normalization property,
$P_G(\cG_n) = \sum_{G\in \cG_n}P_G(G)=1$.

In Definition \ref{def:SSBM}, we have supposed that the node label $X$ is given instead of uniformly distributed random variable
in other literature. Since maximal posterior estimator is equivalent to maximum likehood when the prior is uniform,
these definition variants are equivalent, and our assumption on $X$ make the following analysis more concise.

Given the SBM, the exact recovery problem can be formally defined as follows:
\begin{definition}[Exact recovery in SBM] \label{def:SSBMR}
Given $X$, the random graph $G$ is drawn under $\SSBM(n,k,p,q)$. If there exists an algorithm that takes
$G$ as input and outputs $\hat{X}$ such that
\begin{equation*}
P_G(\hat{X} \in S_k(X)) \to 1 \textrm{ as } n \to \infty
\end{equation*}
\end{definition}

In the above definition, we have used the notation $\hat{X} \in S_k(X)$, which means that we can only
expect a recovery up to a global permutation of the ground truth label vector $X$. This is common in unsupervised
learning as no anchor exists to assign labels to different communities.

For constant $p,q$, that is, $p,q$ is irrelevant with the graph size $n$,
we can always find algorithms to recover $X$ such that the detection error decreases exponentially with $n$.
That is to say, the task with dense graph is relatively easy to handle. Within the paper, we consider a case
when $p = \A, q = \B$. This case corresponds to the sparsest graph when exact recovery of SBM is possible.
And under this condition, a well known result states that
exact recovery is possible if and only if $\sqrt{a} - \sqrt{b} > \sqrt{k}$ \cite{abbe2015community}.

Now we have defined SBM and its exact recovery problem, the definition of Ising model on a graph genered by SBM is given
as follows:
\begin{definition}[Ising model with $k$ states]\label{def:ising}
	The Ising model on a graph $G$ sampled from $\SSBM(n,k,\A,\B)$ with parameters $\gamma,\beta>0$ is a probability distribution on the configurations $\sigma\in W^n$ such that
	\begin{align} \label{eq:isingma}
	P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{\exp(-\beta H(\bar{\sigma}))}{Z_G(\alpha,\beta)}
	\end{align}
	where
	\begin{equation}
	H(\bar{\sigma}) = \gamma \frac{\log n}{n} \sum_{\{i,j\}\not\in E(G)} \delta(\bar{\sigma}_i, \bar{\sigma}_j)
	- \sum_{\{i,j\}\in E(G)} \delta(\bar{\sigma}_i, \bar{\sigma}_j)
	\end{equation}	
	The subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and
	$Z_G(\alpha,\beta)$ is the normalizing constant for this distribution.
\end{definition}
In physics, we often call $\beta$ the inverse temperature and $Z_G(\gamma, \beta)$ the partition function.
The Hamiltonian energy $H(\bar{\sigma})$ consists of two terms: the repulsive interaction between nodes without edge connection
and the attractive interaction between nodes with edge connection. The term $\gamma$ is the ratio of the power of these two
interactions. We should add $\frac{\log n}{n}$ to balance the two terms because there are only $O(\frac{n}{\log n})$
connecting edges for each node.
The probability of each configuration is proportional to $\exp(-\beta H(\bar{\sigma}))$, and the configuration with largest
probability corresponds to the lowest energy.

There are two main difference of Definition \ref{def:ising} with the classical one. Firstly we add a compulsory term
between nodes without an edge connection. This makes these nodes have larger probability to take different labels.
Secondly, we allow the state at each node to take $k$ values from $W$.
When $\alpha = 0$ and $k=2$, Definition \ref{def:ising}
reduces to the classical definition up to a scaling factor.

The above definition of Ising model can be treated a probability distribution conditioned on $G$.
If we take an average over all graph sampled from SBM, we can get the marginal distribution for the state vector $\sigma$.
This is exactly what Stochastic Ising Block Model (SIBM) model says:
\begin{definition}[Stochastic Ising Block Model]
	Given a label $X$, $P_G$ is a distribution given by $\SSBM(n,k,\A,\B)$, Stochastic Ising Block Model
	is a probability distribution on $V$ such that
\begin{equation}\label{eq:sibm}
P_{\SIBM}(\sigma = \bar{\sigma}) = \sum_{G \in \cG_n} P_G(G) P_{\sigma | G}(\sigma = \bar{\sigma}) 
\end{equation}
\end{definition}
Having defined SIBM, we may wonder how the distribution in \eqref{eq:sibm} looks like. Is it concentrated around $X$ or
$\mathbf{1}_n$? The following theorem gives a precise answer to this question.
\begin{theorem}\label{thm:phase_transition}
Define the function $g(\beta), \tilde{g}(\beta)$ as follows:
\begin{equation}
g(\beta) = \frac{be^{\beta} + a e^{-\beta}}{k} - \frac{a+b}{k} +1
\end{equation}
and
\begin{equation}
\tilde{g}(\beta) = \begin{cases}
g(\beta) & \beta \leq \bar{\beta} \\
g(\bar{\beta}) & \beta > \bar{\beta}
\end{cases}
\end{equation}
where $\bar{\beta} =  \arg\min_{\beta > 0} g(\beta)$.
Let $\beta^*$ be the solution to the equation $g(\beta) = 0$ and $\beta^* < \bar{\beta}$, then depending on
how $(\gamma, \beta)$ take values, $\forall \epsilon > 0$, when $n$ is sufficiently large, we have
\begin{enumerate}
\item If $\gamma > b$ and $\beta > \beta^*$,  $P_{\SIBM}(\sigma \not\in S_k(X)) \leq n^{\tilde{g}(\beta)/2 + \epsilon}$;
\item If $\gamma > b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma \in S_k(X)) \leq (1+o(1))\min\{n^{g(\bar{\beta})},
n^{-g(\beta)+ \epsilon} \}$;
\item If $\gamma < b$ and $\beta > \beta^*$, $P_{\SIBM}(\sigma \not\in \Lambda) \leq (1+o(1))n^{\tilde{g}(\beta)/2}$;
\item If $\gamma < b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma \in \Lambda) \leq (1+o(1))\min\{n^{g(\bar{\beta})},
n^{-g(\beta)} \}$;
\end{enumerate}
\end{theorem}
By simple calculus, $\tilde{g}(\beta) < 0$ for $\beta> \beta^*$ and $g(\beta)>0$ for $\beta < \beta^*$.
As $n \to \infty$, the probability mentioned in Theorem \ref{thm:phase_transition} converges to $0$ at least
in polynomial speed.
Therefore, Theorem \ref{thm:phase_transition} establishes the sharp phase transition property of SIBM model.
Generally speaking, there are two transition thresholds on the $(\gamma, \beta)$ plane. To illustrate Theorem
\ref{thm:phase_transition} more clearly,
let $D(\sigma, \sigma')$ be the event when $\sigma$ is nearest to $\sigma'$ among all its permutation.
That is
\begin{equation}
D(\sigma, \sigma') := \{ \sigma = \arg\min_{f \in S_k} \dist(f(\sigma), \sigma')  \}
\end{equation}
Then Theorem 1 can be stated in the following way:
\begin{corollary}\label{cor:phase4}
\begin{enumerate}
	\item If $\gamma > b$ and $\beta > \beta^*$, $P_{\SIBM}(\sigma = X | D(\sigma, X))  = 1-o(1)$;
	\item If $\gamma > b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma = X | D(\sigma, X))  = o(1)$;
\item If $\gamma < b$ and $\beta > \beta^*$, $P_{\SIBM}(\sigma = \mathbf{1} | D(\sigma,  \mathbf{1}))  = 1-o(1)$;	
	\item If $\gamma < b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma = \mathbf{1}| D(\sigma,  \mathbf{1}))  =  o(1)$;
\end{enumerate}	
\end{corollary}
Corollary \ref{cor:phase4} is illustrated in Diagram.

The rest of this section is devoted to the proof of Theorem \ref{thm:phase_transition}, which relies on the
careful analysis on the one-flip energy difference. This useful result is summarized in the following lemma:
\begin{lemma}\label{lem:lemmaDiff}
	Suppose $\bar{\sigma}'$ differs from $\bar{\sigma}$ only at position $r$ by $\bar{\sigma}'_r = \omega^s \cdot \bar{\sigma}_r$.
	Then the change of energy is
	\begin{align}
	H(\bar{\sigma}') - H(\bar{\sigma}) &= (1+\gamma \frac{\log n}{n})\sum_{i \in N_r(G)} J_s(\bar{\sigma}_r, \bar{\sigma}_i)
	\notag \\
	&- \gamma \frac{\log n}{n} (m(\bar{\sigma}_r)-m(\omega^s \cdot \bar{\sigma}_r)-1) 
	\end{align}
	where $m(\omega^j) := |\{i \in [n] | \bar{\sigma}_i = \omega^j | \}$ and $J_s(x, y) = \delta(x, y) - \delta(\omega_s \cdot x, y)$
\end{lemma}
Lemma \ref{lem:lemmaDiff} gives an explicit way to compare the probability of two configurations by the following
equality:
\begin{equation}\label{eq:Pratio}
\frac{P_{\sigma |G } (\sigma = \bar{\sigma}')}{P_{\sigma |G } (\sigma = \bar{\sigma})}
= \exp(-\beta(H(\bar{\sigma}') - H(\bar{\sigma})))
\end{equation}
Besides, since the graph is sparse and every node has $O(\log n)$ neighbors, the computational cost for the energy difference
is also $O(\log n)$. 

When $H(\bar{\sigma}') < H(\bar{\sigma})$, we can expect $P_{\sigma | G}(\sigma = \bar{\sigma}')$ is far less than 
$P_{\sigma | G}(\sigma = \bar{\sigma})$. However,
since $G$ is sampled from SBM, the righthand side of \eqref{eq:Pratio} is a random variable. The stochastic behaviour
of $ \exp(-\beta(H(\bar{\sigma}') - H(\bar{\sigma}))) $ plays an important role when establishing Theorem \ref{thm:phase_transition}.
The case $\gamma > b$ has been analyzed in great detail in \cite{sibmmc}, here we enhance the existing result in the following lemma:
\begin{lemma}\label{lem:specialCase}
If $\gamma> b$ and $\beta > \beta^*$, there exists a set $\cG'_n$ such that
$P_G(\cG'_n) \geq 1 - (1+o(1))n^{\tilde{g}(\beta)/2}$ and for every $G\in \cG'_n$
\begin{equation}
\frac{P_{\sigma |G } (\sigma = \bar{\sigma}')}{P_{\sigma |G } (\sigma = X)} \leq n^{\tilde{g}(\beta)/2-1}
\end{equation}
where $\dist(\bar{\sigma}',X)=1$.
\end{lemma}
\begin{proof}
	From \eqref{eq:Pratio}, we only need to show that
	\begin{equation}\label{eq:beta_HP}
	P_G( \exp(-\beta(H(\bar{\sigma}') - H(\bar{\sigma}))) \geq n^{\tilde{g}(\beta)/2-1}) \leq (1+o(1))n^{\tilde{g}(\beta)/2}
	\end{equation}
	Taking $\bar{\sigma}=X$ in Lemma \ref{lem:lemmaDiff}, we have
	\begin{equation}\label{eq:energy_diff}
	H(\bar{\sigma}') - H(\bar{\sigma}) = (1+\gamma \frac{\log n}{n})(A^0_r - A^s_r) + \gamma\frac{\log n}{n}
	\end{equation}
	where $A_r^0 \sim Binom(\frac{n}{k}-1, \A)$ and $A^s_r \sim Binom(\frac{n}{k}, \B)$.
	Therefore, \eqref{eq:beta_HP} is equivalent with
	\begin{equation}\label{eq:pgAs}
	P_G\big(A^s_r - A_r^0 \geq t\log n\big)
	\leq (1+o(1))n^{\tilde{g}(\beta)/2}
	\end{equation}
	where $t=(1+o(1))\frac{1}{\beta}(\frac{\tilde{g}(\beta)}{2} -1)>\beta\frac{b-a}{k}+1$ and the $o(1)$ term is not random.
	\eqref{eq:pgAs} follows directly from Lemma 1 of \cite{sibmmc}.
\end{proof}
Since 
$\frac{P_{\sigma |G } (\dist(\sigma, X)=1)}{P_{\sigma |G } (\sigma = X)} = \sum_{s=1}^{k-1}
\sum_{r=1}^n \frac{P_{\sigma |G } (\sigma = \bar{\sigma}'(s,r))}{P_{\sigma |G } (\sigma = X)}$ 
where $\bar{\sigma}'(s,r)$ only differs from $X$ by $\bar{\sigma}'_r = \omega^s \cdot X_r$.
In such a case, we can show that there exists a set $\cG'_n$
$P_G(\cG'_n) \geq 1 - (1+o(1))n^{\tilde{g}(\beta)/2}$ and 
$\frac{P_{\sigma |G } (\dist(\sigma, X)=1)}{P_{\sigma |G } (\sigma = X)} \leq n^{\tilde{g}(\beta)/2}$.
Or more generally, we have the following proposition
\begin{proposition}\label{prop:small}
If $\gamma>b$, $\beta>\beta^\ast$,
For $1\leq r \leq \frac{n}{\sqrt{\log n}}$
and $\forall \epsilon > 0$, there is a set $\cG^{(r)}$ such that
\begin{equation}\label{eq:Gr}
P_G(\cG^{(r)}_n) \ge 1 - n^{r(\tilde{g}(\beta)/2 + \epsilon)}
\end{equation}
and
for every $G\in\cG^{(r)}_n$,
\begin{equation}\label{eq:psigmaX}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r | D(\sigma, X))}
{P_{\sigma|G}(\sigma=X | D(\sigma, X))} <
n^{r \tilde{g}(\beta) /2}
\end{equation}
For $r> \frac{n}{\sqrt{\log n}}$, there is a set $\cG^{(r)}$ such that
\begin{equation}\label{eq:Gr1}
P(G\in\cG^{(r)}_n) \ge 1 - e^{-n}
\end{equation}
and
for every $G\in\cG^{(r)}_n$,
\begin{equation}\label{eq:psigmaX1}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r | D(\sigma, X))}
{P_{\sigma|G}(\sigma=X | D(\sigma, X))} <
e^{-n}
\end{equation}
\end{proposition}
We may tend to use Lemma \ref{lem:specialCase} to establish Proposition \ref{prop:small}. However, the set $\cG_n'$ may depend
on $(i,r)$ and it is not a good idea to use union bound to get the proper set. The rigorous proof relies on
proper choice of $\cG_n^{(r)}$
and is given in
the appendix.

Now we use Proposition \ref{prop:small} to show 1) of Theorem \ref{thm:phase_transition}.
\begin{proof}[Proof of Theorem \ref{thm:phase_transition}]
Since $P_{\SIBM}(\sigma \not \in S_k(X)) = \sum_{f\in S_k} P_{\SIBM}(\sigma \neq f(X) | D(\sigma, f(X))) P_{\SIBM}(D(\sigma, f(X)))$,
we only need to establish $P_{\SIBM}(\sigma \neq X | D(\sigma, X)) \leq k n^{\tilde{g}(\beta)/2 + \epsilon}$.
Let $\cG'_n = \cap_{r=1}^n \cG_n^{(r)}$, choose $\frac{\epsilon}{2}$, from \eqref{eq:Gr} we have
\begin{align*}
P_G(\cG'^c_n) &= P_G(\cup_{r=1}^n (\cG_n^{(r)})^c) \\
&\leq \sum_{r=1}^{n/\sqrt{\log n } } n^{r(\tilde{g}(\beta)/2 + \epsilon/2)}  + n e^{-n} \\
& \leq \frac{1}{2} n^{\tilde{g}(\beta)/2 + \epsilon}
\end{align*}
where the last equality follows from the estimation of sum of geometric series.
On the other hand, for every $G \in \cG'_n$, from \eqref{eq:psigmaX}
we have
\begin{align*}
\frac{P_{\sigma | G}(\sigma \neq X | D(\sigma, X))}{1-P_{\sigma | G}(\sigma \neq X | D(\sigma, X))} &= \frac{P_{\sigma | G}(\sigma \neq X | D(\sigma, X))}{P_{\sigma|G}(\sigma=X | D(\sigma, X))} \\
&< \sum_{r=1}^{n/\sqrt{\log n }}  n^{r\tilde{g}(\beta)/2} + n e^{-n}
\end{align*}
from which we can get the estimation $P_{\sigma | G}(\sigma \neq X | D(\sigma, X))\leq \frac{1}{2}n^{\tilde{g}(\beta)/2 + \epsilon}$.
Finally, 
\begin{align*}
P_{\SIBM}(\sigma \neq X|D(\sigma, X)) &= \sum_{G\in \cG'_n} P_G(G)P_{\sigma |G}(\sigma \neq X | D(\sigma, X)) \\
+ P_G(\cG'^c_n)
& \leq n^{\tilde{g}(\beta)/2 + \epsilon}
\end{align*}

\end{proof}
If $\gamma > b$ and $\beta < \beta^*$, we have the following proposition:
\begin{proposition}\label{prop:large2}
	If $\gamma > b$ and $\beta < \beta^*$, there is a set $\cG'_n$ such that
	\begin{equation}
	P_G(\cG'_n) \geq 1 - (1+o(1))n^{g(\bar{\beta})}
	\end{equation}
	and for every $G \in \cG'_n$,
	\begin{equation}\label{eq:diff1g}
	\frac{P_{\sigma|G}(\dist(\sigma, X)=1 | D(\sigma, X))}
	{P_{\sigma|G}(\sigma=X | D(\sigma, X))} \geq (1+o(1))n^{g(\beta_n)}
	\end{equation}
	where $\beta_n = \beta(1+\gamma\frac{\log n}{n})$.
\end{proposition}
We will use Proposition \ref{prop:large2} to establish 2) of Theorem \ref{thm:phase_transition}.
\begin{proof}
	Using Proposition \ref{prop:large2}, for every $G \in \cG'_n$
	we can get
	$$
	\frac{1-P_{\sigma | G}(\sigma=X | D(\sigma, X))}{P_{\sigma | G}(\sigma=X | D(\sigma, X))}\geq (1+o(1))n^{g(\beta_n)}
	$$
	We then have
	$$
	P_{\sigma | G}(\sigma=X| D(\sigma, X)) \leq (1+o(1)) n^{-g(\beta_n)}
	$$
	Then
	\begin{align*}
	P_{\SIBM}(\sigma=X| D(\sigma, X)) & \leq P(\cG'^c_n) + \sum_{G\in \cG'_n}P_G(G) P_{\sigma|G}(\sigma=X| D(\sigma, X))\\
	& \leq (1+o(1))n^{g(\bar{\beta})} + (1+o(1)) n^{-g(\beta_n)}\\
	& \leq (1+o(1)) \min\{n^{g(\bar{\beta})},n^{-g(\beta) + \epsilon}  \}
	\end{align*}
\end{proof}
\section{Degree-corrected Stochastic Ising Block Model}\label{sec:dcsibm}
\section{Parameter Estimation of SBM}\label{sec:psbm}
\section{Community Detection based on Metropolis Sampling}\label{sec:ms}
\section{Conclusion}
\bibliographystyle{IEEEtran}
\bibliography{exportlist}
\appendix
\section*{Proof of Proposition \ref{prop:small}}
We distinguish the discussion between two cases: $r\leq \frac{n}{\sqrt{\log n}}$
and $r > \frac{n}{\sqrt{\log n}}$. Some additional notation is needed for the proof.
Similar with \eqref{eq:energy_diff}, generally we can write
$$
H(\bar{\sigma}) - H(X)=
(1 + \gamma \frac{ \log n}{n})[A_{\bar{\sigma}} - B_{\bar{\sigma}}] + \gamma\frac{ \log n}{n} N_{\bar{\sigma}}
$$
in which we use $A_{\bar{\sigma}}$ (or $B_{\bar{\sigma}}$) to represent the binomial random variable with parameter $\A$ (or $\B$)
respectively and $N_{\bar{\sigma}}$ is a deterministic positive number, irrelevant with the graph structure.

When $r\leq \frac{n}{\sqrt{\log n}}$, we can show that $\dist(\sigma, X) = r$ implies $D(\sigma, X)$ by using the triangle
inequality of $\dist$. For $f \in S_k \backslash \{ id \}$, we have
$$
\frac{2n}{k} \leq \dist(f(X), X) \leq \dist(\sigma, f(X)) + \dist(\sigma, X)
$$
Therefore, $\dist(\sigma, f(X)) \geq \frac{2n}{k} - \frac{n}{\sqrt{\log n}} \geq \dist(\sigma, X)$ and
\eqref{eq:psigmaX} is equivalent with
\begin{equation}\label{eq:psigmaX2}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r)}
{P_{\sigma|G}(\sigma=X)} <
n^{r \tilde{g}(\beta) /2}
\end{equation}
The left hand side can be written as:
\begin{align*}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r)}
{P_{\sigma|G}(\sigma=X)}  &= \sum_{\dist(\bar{\sigma}, X)=r}\exp(-\beta(H(\bar{\sigma})-H(X)))\\
&\leq \sum_{\dist(\bar{\sigma}, X)=r}\exp(\beta_n(B_{\bar{\sigma}}-A_{\bar{\sigma}}))
\end{align*}


Define $\Xi_n(r): = \sum_{\dist(\bar{\sigma}, X)=r}\exp(\beta_n(B_{\bar{\sigma}}-A_{\bar{\sigma}}))$ and we only need to show that
\begin{equation}
P_{G}(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2}) \leq  n^{r (\tilde{g}(\beta) /2 + \epsilon)}
\end{equation}
Define the event $\Lambda_n(G,r):=\{B_{\bar{\sigma}} -A_{\bar{\sigma}} < 0, \forall \dist(\bar{\sigma}, X)=r\}$,
we proceed as follows:
\begin{align*}
P_{G}(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2}) &\leq
P_G(\Lambda_n(G,r)^c) \\
&+ P_G(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2} |\Lambda_n(G,r) )
\end{align*}
For the first term, 
$P_G(\Lambda_n(G,r)^c) \leq n^{rg(\bar{\beta})} \leq n^{r (\tilde{g}(\beta) /2 + \epsilon/2)}$.
For the second term, we use Markov inequality:
\begin{align*}
P_G(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2} |\Lambda_n(G,r) )
\leq \mathbb{E}[\Xi_n(r)|\Lambda_n(G,r)]n^{-r \tilde{g}(\beta) /2} 
\end{align*}
The conditional expectation can be estimated as follows:
\begin{align*}
&\mathbb{E}[\Xi_n(r)|\Lambda_n(G,r)]=
\sum_{\dist(\bar{\sigma}, X) = r}\sum_{tr\log n = -\infty }^{-1} \\
& P_G(B_{\bar{\sigma}} -A_{\bar{\sigma}}=tr\log n)\exp(\beta_n tr \log n) \\
& \leq (k-1)^r n^{r+r\beta(b-a)/k} +
\sum_{\dist(\bar{\sigma}, X) = r}\sum_{tr\log n = r(b-a)/k\log n }^{-1} \\
& P_G(B_{\bar{\sigma}} -A_{\bar{\sigma}}=t\log n)\exp(\beta_n rt \log n)
\end{align*}
$r+r\beta(b-a)/k = f_{\beta}(\frac{b-a}{k}) < \tilde{g}(\beta)$, therefore,
$(k-1)^r n^{r+r\beta(b-a)/k}n^{-r \tilde{g}(\beta) /2} \leq n^{r (\tilde{g}(\beta) /2 + \epsilon/2)} $.
Using Lemma 6 in \cite{sibmmc}, we have
\begin{align*}
P_G(B_{\bar{\sigma}} -A_{\bar{\sigma}}=t\log n)\exp(\beta_n rt \log n) \leq 
n^{r(f_{\beta_n}(t)-1 + O(\frac{1}{\sqrt{\log n}}))}
\end{align*}
Since $\beta_n \to \beta$, $\forall \epsilon$, when $n$ is sufficiently large
we have $\tilde{g}(\beta_n) \leq \tilde{g}(\beta) + \epsilon /2$.
Therefore,
\begin{align*}
&\sum_{\dist(\bar{\sigma}, X) = r}\sum_{\substack{tr\log n = \\ r(b-a)/k\log n} }^{-1}
P_G(B_{\bar{\sigma}} -A_{\bar{\sigma}}=t\log n)\exp(\beta_n rt \log n) \\
& \leq  n^{r(\tilde{g}(\beta_n) - \tilde{g}(\beta)/2)}\\
& \leq  n^{r(\tilde{g}(\beta)/2 + \epsilon/2)} O(\log n) (k-1)^r
\end{align*}
Combining the above equations, we have
\begin{align*}
P_{G}(\Xi_n(r) \geq n^{r \tilde{g}(\beta) /2}) &\leq  n^{r(\tilde{g}(\beta)/2 + \epsilon/2)} O(\log n) (k-1)^r\\
&\leq n^{r(\tilde{g}(\beta)/2 + \epsilon)}
\end{align*}
When $r>\frac{n}{\sqrt{\log n}}$, using Lemma 5 from \cite{sibmmc}, we can choose a sufficiently large constant $C>1$
such that $k^n\exp(-Cn) < e^{-n}$
\begin{align*}
\frac{P_{\sigma|G}(\dist(\sigma, X)=r | D(\sigma, X))}
{P_{\sigma|G}(\sigma=X | D(\sigma, X))} &= \sum_{\substack{D(\sigma, X) \\ \dist(\sigma, X)=r}} \frac{P_{\sigma | G}(\sigma = \bar{\sigma}) }{P_{\sigma | X}(\sigma = X)} \\
&> \exp(-Cn)
\end{align*}
happens with probability less than $e^{-n}$. Therefore, \eqref{eq:psigmaX1} holds.

\section*{Proof of Proposition \ref{prop:large2}}
The left hand of \eqref{eq:diff1g} can be rewritten as:
\begin{equation}\label{eq:knd}
	\frac{P_{\sigma|G}(\dist(\sigma, X)=1)}
{P_{\sigma|G}(\sigma=X)}= (1+o(1))\sum_{s=1}^{k-1}\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0))
\end{equation}
From Proposition 7, there is a set $\cG^{(1)}_n$ such that $P_G(\cG^{(1)}_n) \geq 1-n^{g(\bar{\beta})}$
and
\begin{align}
\mathbb{E}[\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0)) | G \in \cG^{(1)}_n] &= (1+o(1))n^{g(\beta_n)} \\
\Var[\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0)) | G \in \cG^{(1)}_n] &= \Theta(n^{2g(\beta_n)-1}\log n)
\end{align}
Let $\cG^{(2)}_n: = \{|\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0)) - (1+o(1))n^{g(\beta_n)}  | \leq n^{g(\beta_n) - \epsilon} \}$,

Using Chebyshev inequality, we have
\begin{equation*}
P_G(G \not\in \cG^{(2)}_n \Big\vert  G \in \cG^{(1)}_n) \leq \Theta(n^{2\epsilon - 1} \log n) 
\end{equation*}
Let $\cG'_n = \cG^{(1)}_n \cap \cG^{(2)}_n$
\begin{align*}
P_G(G \in \cG'_n) &= P_G(\cG^{(1)}_n) P_G(G \in \cG_n^{(2)} | G \in \cG_n^{(1)}) \\
& \geq (1-\Theta(n^{2\epsilon - 1} \log n))(1-n^{g(\beta_n)}) \\
&= 1-(1+o(1))n^{g(\beta_n)}
\end{align*}
and for every $G\in\cG'_n$,
\begin{equation*}
\sum_{r=1}^n \exp(\beta_n (A_r^s - A_r^0)) = (1+o(1)) n^{g(\beta_n)}
\end{equation*}
Therefore, from \eqref{eq:knd} we have
\begin{equation*}
	\frac{P_{\sigma|G}(\dist(\sigma, X)=1)}
{P_{\sigma|G}(\sigma=X)} \geq (1+o(1)) n^{g(\beta_n)}
\end{equation*}
\end{document}