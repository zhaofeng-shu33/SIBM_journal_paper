\documentclass[journal]{IEEEtran}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newcommand{\A}{\frac{a \log(n)}{n}}
\newcommand{\B}{\frac{b \log(n)}{n}}
\newcommand{\cG}{\mathcal{G}}
\DeclareMathOperator{\SSBM}{SSBM}
\DeclareMathOperator{\SIBM}{SIBM}
\DeclareMathOperator{\dist}{dist}

\title{Exact recovery of Stochastic Block Model with Ising sampling}
\author{
	Feng Zhao,~\IEEEmembership{Student Member, IEEE}\\
	Min Ye,~\IEEEmembership{Member, IEEE} and
	Shao-Lun~Huang,~\IEEEmembership{Member, IEEE}\\
	\thanks{Feng Zhao is with the
		Department of Electrical Engineering, Beijing, China.
		(Email: zhaof17@mails.tsinghua.edu.cn).
		Min Ye and S-L.~Huang are with the Data Science and Information
		Technology Research Center, Tsinghua-Berkeley Shenzhen Institute,
		Shenzhen, China (Email: \{yeemmi, shaolun.huang\}@sz.tsinghua.edu.cn).
	}}
	
\begin{document}
	\maketitle
\begin{abstract}
	Based on Ising sampling, we propose a stochastic algorithm to achieve the exact recovery for stochastic block model (SBM).
	The stochastic algorithm can be transformed to an optimization problem, which includes maximum likelihood and maximal modularity.
	Besides, we give an unbiased convergent estimator of the parameters of SBM, which can be computed in constant time.
	Finally, we use metropolis sampling to realize the theoretical Ising sampling and demonstrates the better performance of our method,
	compared with other theoretically guaranteed algorithm.
\end{abstract}
\begin{IEEEkeywords}
	stochastic block model, exact recovery, Ising model, modularity maximization, Metropolis sampling
\end{IEEEkeywords}
\section{Introduction}
Stochastic Block Model (SBM) is one of statistical modeling for community detection problems  \cite{holland1983stochastic, abbe2017community}.
It provides benchmark artificial dataset to evaluate different community detection algorithms.
Besides, SBM also inspires the design of algorithm for community detection tasks. These algorithms, such as
semi-definite relaxation, spectral clustering and label propagation, not only have theoretical guarantee when applied to SBM,
but perform well on dataset without SBM assumption. The study on the theoretical guarantee on SBM model can be divided between
exact recovery and partial recovery. For both cases, the asymptotic behavior of detection error
is analyzed when the scale of graph tends to infinity. There are already some well-known results of the exact recovery problem
on SBM.	To name but a few, Abbe and Mossel established the exact recovery region for a special sparse SBM with two communities  \cite{abbe2015exact, mossel2016}.
Later on, the result is extended to general SBM with multiple communities \cite{abbe2015community}.

Besides theoretical study on SBM, in community detection maximal modularity is a popular detection method \cite{Newman8577}.
The modularity is a kind of criteria and object function. Though maximal modularity works well in many practical problems, it's
generally unknown whether it can achieve exact recovery for SBM model. Newman, the inventor of modularity, demonstrates that
the modularity method is equivalent with maximum likelihood for a degree-corrected SBM model \cite{newman2016equivalence}. In this article,
we will fill the gap to prove that maximal modularity can achieve exact recovery for symmetric SBM model.

Our analysis of maximal modularity is based on Ising model, which is a probability distribution of node states \cite{ising1925beitrag}.
Ising model is originally proposed in statistical mechanics to model the ferromagnetism phenomenon but has widely application in neuroscience, information theory
and social networks. Among different variants of Ising models, the phase transition property is shared. Based on the random graph generated by SBM with two underlining communities,
the Ising model is first studied by \cite{ye2020exact}. Our work will extend the existing result to multiple community case and establish the phase transition
property and sample complexity results. Then we will propose a specialized Ising model using the definition of modularity. Sampling from this model,
we can also achieve exact recovery for SBM.

In order to achieve exact recovery of SBM, the extra parameters of Ising model should be carefully chosen, which depends on the parameters of SBM. 
Previous methods require jointly estimation of node labels and model parameters \cite{nowicki2001estimation}, which are not suitable when only model parameters are required.
In this article, we propose an unbiased convergent estimator for SBM parameters when the number of communities is given. This estimator is not only useful
for Ising sampling, but also beneficial for other recovery algorithms which requires SBM parameters.

Exact solution to maximize the modularity or exact sampling from modularity-based Ising model is NP hard. Many algorithms have been proposed to find an approximation of maximal modularity in polynomial time.
Among these algorithms, simulated annealing performs well and produces a solution very close to the true maximal value \cite{liu2010detecting}.
On the other hand, in original Ising model,
metropolis sequential sampling is used to generate samples for Ising model \cite{metropolis1953equation}. Simulated annealing and metropolis sampling are closely related. In this article, we will
use metropolis sampling technique to sample from Ising model on SBM. We will demonstrate by experiments that for non-exact recovery region of SBM our method outperforms other
theoretically guaranteed algorithm like SDP and spectral clustering.

This paper is organized as follows. Firstly, in section \ref{sec:sibm} we introduce the stochastic Ising block model for both two and multiple communities.
Then in the next section \ref{sec:dcsibm}, we propose Degree-corrected Stochastic Ising Block Model and uses this model to prove the exact recovery by modularity
maximization algorithm. Besides, in section \ref{sec:psbm}, we give a parameter estimator for SBM. Based on this estimator, in section \ref{sec:ms},
we propose a community detection method which use metropolis algorithm to generate sample for Ising model. Numerical experiments and conclusion are given at last
to finish this paper.

Throughout this paper, the number of community is denoted by $k$; $m$ is the number of samples; $\lfloor x \rfloor$ is the floor function of $x$; the random undirected graph $G$ is written as $G(V,E)$ with vertex set $V$ and edge set $E$;
$V=\{1,\dots, n\} =: [n]$;
the label of each node is a random variable $X_i$; $X_i$ is chosen from $W= \{1, \omega, \dots, \omega^{k-1}\}$ and we further require $W$
is a cyclic group with order $k$; $W^n$ is the n-ary Cartesian power of $W$;
$f$ is a permutation function on $W$ and applied to $W^n$ in elementwise way;
the set $S_k$ is used to represent all permutation functions on $W$ and $S_k(\sigma):=\{f(\sigma)| f\in S_k\}$ for $\sigma \in W^n$;
the indicator function $\delta(x,y)$ is defined as
$\delta(x,y) = 1 $ when $x=y$, and $\delta(x,y)=0$ when $x\neq y$;
$g(n) = \Theta(f(n))$ if there exists constant $c_1 < c_2$ such that $c_1 f(n) \leq g(n) \leq c_2 f(n)$
for large $n$;
$\Lambda := \{ \omega^j  \cdot \mathbf{1}_n | j=0, \dots,k-1\}$
where $\mathbf{1}_n$ is the all one vector with dimension $n$;
we define the distance of two vectors as:
$\dist(\sigma, X)
=|\{i\in[n]:\sigma_i\neq X_i\}| \textrm{ for } \sigma,X\in W^n
$ and the distance of a vector to a space $S\subseteq W^n$
as
$\dist(\sigma,S)
:=\min\{\dist(\sigma, \sigma') | \sigma' \in S\}
$.
\section{Related works}
The classical Ising model is defined on lattice and confined to two state $\{\pm 1\}$. This definition
can be extended to general graph and multiple state \cite{potts1952some}. In \cite{liu2017log}, Liu considered
the Ising model defined on graph generated by sparse SBM and his focus is to compute the log partition function,
which is averaged over the random graphs. In \cite{berthet2019exact}, an Ising model with repulsive interaction
is considered on a fixed graph structure, and the phase transition involving both the assortative and disassortative
parameters.
% our contribution and main results

\section{Stochastic Ising Block Model}\label{sec:sibm}
We consider a special symmetric stochastic block model, which is defined as follows:
	\begin{definition}[SSBM with $k$ communities] \label{def:SSBM}
	Let $0\leq q<p\leq 1$, $V=[n]$ and $X=(X_1,\dots,X_n)\in W^n$. $X$ satisfies the constraint that $|\{v \in [n] : X_v = u\}| = \frac{n}{k}$ for $u\in W$.
	The random graph $G$ is generated under $\SSBM(n,k,p,q)$ if
	\begin{enumerate}
	\item There is an edge of $G$ between the vertices $i$ and $j$ with probability $p$ if $X_i=X_j$ and with probability $q$ if $X_i \neq X_j$
	\item The existence of each edge is independent with each other.
	\end{enumerate}
\end{definition}
To explain the generation of SSBM in more detail,
we use $Z_{ij}:=\mathbbm{1}[\{i,j\} \in E(G)]$, which is the indicator function of the existence of an edge between node $i$ and $j$.
Given the node labels $X$, $Z_{ij}$ is a Bernoulli random variable, whose expectation is given by:
\begin{equation}
\mathbb{E}[Z_{ij}] =
\begin{cases}
p & X_i = X_j \\ 
q & X_i \neq X_j
\end{cases}
\end{equation}
Then the random graph $G$ is completely specified by $\{Z_{ij}, 1\leq i<j\leq n\}$ in which all $Z_{ij}$ are jointly independent.
The probability distribution for SBM can be written as:
\begin{align}
&P_G(Z_{ij} = z_{ij}, 1\leq i<j\leq n) = p^{\sum_{X_i = X_j} z_{ij}}q^{\sum_{X_i \neq X_j} z_{ij}} \cdot \notag \\
&\quad (1-p)^{\sum_{X_i = X_j} (1-z_{ij})}(1-q)^{\sum_{X_i \neq X_j} (1-z_{ij})}
\end{align}
We will use the notation $\cG_n$ to represent a set containing all graphs with $n$ nodes. By the normalization property,
$P_G(\cG_n) = \sum_{G\in \cG_n}P_G(G)=1$.

In Definition \ref{def:SSBM}, we have supposed that the node label $X$ is given instead of uniformly distributed random variable
in other literature. Since maximal posterior estimator is equivalent to maximum likehood when the prior is uniform,
these definition variants are equivalent, and our assumption on $X$ make the following analysis more concise.

Given the SBM, the exact recovery problem can be formally defined as follows:
\begin{definition}[Exact recovery in SBM] \label{def:SSBMR}
Given $X$, the random graph $G$ is drawn under $\SSBM(n,k,p,q)$. If there exists an algorithm that takes
$G$ as input and outputs $\hat{X}$ such that
\begin{equation*}
P_G(\hat{X} \in S_k(X)) \to 1 \textrm{ as } n \to \infty
\end{equation*}
\end{definition}

In the above definition, we have used the notation $\hat{X} \in S_k(X)$, which means that we can only
expect a recovery up to a global permutation of the ground truth label vector $X$. This is common in unsupervised
learning as no anchor exists to assign labels to different communities.

For constant $p,q$, that is, $p,q$ is irrelevant with the graph size $n$,
we can always find algorithms to recover $X$ such that the detection error decreases exponentially with $n$.
That is to say, the task with dense graph is relatively easy to handle. Within the paper, we consider a case
when $p = \A, q = \B$. This case corresponds to the sparsest graph when exact recovery of SBM is possible.
And under this condition, a well known result states that
exact recovery is possible if and only if $\sqrt{a} - \sqrt{b} > \sqrt{k}$ \cite{abbe2015community}.

Now we have defined SBM and its exact recovery problem, the definition of Ising model on a graph genered by SBM is given
as follows:
\begin{definition}[Ising model with $k$ states]\label{def:ising}
	The Ising model on a graph $G$ sampled from $\SSBM(n,k,\A,\B)$ with parameters $\alpha,\beta>0$ is a probability distribution on the configurations $\sigma\in W^n$ such that
	\begin{align} \label{eq:isingma}
	&P_{\sigma|G}(\sigma=\bar{\sigma})=\frac{1}{Z_G(\alpha,\beta)}
	\exp\Big(\beta \sum_{\{i,j\}\in E(G)}\delta(\bar{\sigma}_i, \bar{\sigma}_j) \notag\\
	&-\frac{\alpha\log(n)}{n} \sum_{\{i,j\}\notin E(G)} \delta(\bar{\sigma}_i, \bar{\sigma}_j)
	\Big)
	\end{align}
	where the subscript in $P_{\sigma|G}$ indicates that the distribution depends on $G$, and
	$Z_G(\alpha,\beta)$ is the normalizing constant for this distribution.
\end{definition}
In physics, we often call $\beta$ the inverse temperature and $Z_G(\alpha, \beta)$ the partition function.
The Hamiltonian energy $H(\bar{\sigma})$ is given by:
\begin{equation}
H(\bar{\sigma}) = \gamma \frac{\log n}{n} \sum_{\{i,j\}\not\in E(G)} \delta(\bar{\sigma}_i, \bar{\sigma}_j)
- \sum_{\{i,j\}\in E(G)} \delta(\bar{\sigma}_i, \bar{\sigma}_j)
\end{equation}
where $\gamma = \frac{\alpha}{\beta}$.

Then the probability of each configuration is proportional to $\exp(-\beta H(\bar{\sigma}))$, and the configuration with largest
probability corresponds to the lowest energy.

There are two main difference of Definition \ref{def:ising} with the classical one. Firstly we add a compulsory term
between nodes without an edge connection. This makes these nodes have larger probability to take different labels.
Secondly, we allow the state at each node to take $k$ values from $W$.
When $\alpha = 0$ and $k=2$, Definition \ref{def:ising}
reduces to the classical definition up to a scaling factor.

The above definition of Ising model can be treated a probability distribution conditioned on $G$.
If we take an average over all graph sampled from SBM, we can get the marginal distribution for the state vector $\sigma$.
This is exactly what Stochastic Ising Block Model (SIBM) model says:
\begin{definition}[Stochastic Ising Block Model]
	Given a label $X$, $P_G$ is a distribution given by $\SSBM(n,k,\A,\B)$, Stochastic Ising Block Model
	is a probability distribution on $V$ such that
\begin{equation}\label{eq:sibm}
P_{\SIBM}(\sigma = \bar{\sigma}) = \sum_{G \in \cG_n} P_G(G) P_{\sigma | G}(\sigma = \bar{\sigma}) 
\end{equation}
\end{definition}
Having defined SIBM, we may wonder how the distribution in \eqref{eq:sibm} looks like. Is it concentrated around $X$ or
$\mathbf{1}_n$? The following theorem gives a precise answer to this question.
\begin{theorem}\label{thm:phase_transition}
Define the function $g(\beta), \tilde{g}(\beta)$ as follows:
\begin{equation}
g(\beta) = \frac{be^{\beta} + a e^{-\beta}}{k} - \frac{a+b}{k} +1
\end{equation}
and
\begin{equation}
\tilde{g}(\beta) = \max\{g(\beta), g(\bar{\beta})\}
\end{equation}
where $\bar{\beta} = \min_{\beta > 0} g(\beta)$.
Let $\beta^*$ be the solution to the equation $g(\beta) = 0$ and $\beta^* < \bar{\beta}$, then depending on
how $(\gamma, \beta)$ take values, we have
\begin{enumerate}
\item If $\gamma > b$ and $\beta > \beta^*$, $P_{\SIBM}(\sigma \not\in S_k(X)) \leq (1+o(1))n^{\tilde{g}(\beta)/2}$;
\item If $\gamma < b$ and $\beta > \beta^*$, $P_{\SIBM}(\sigma \not\in \Lambda) \leq (1+o(1))n^{\tilde{g}(\beta)/2}$;
\item If $\gamma > b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma \in S_k(X)) \leq (1+o(1))\min\{n^{g(\bar{\beta})},
n^{-g(\beta)} \}$;
\item If $\gamma < b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma \in \Lambda) \leq (1+o(1))\min\{n^{g(\bar{\beta})},
n^{-g(\beta)} \}$;
\end{enumerate}
\end{theorem}
Theorem \ref{thm:phase_transition} has established the sharp phase transition property of SIBM model.
Generally speaking, there are two transition thresholds on the $(\gamma, \beta)$ plane.
Let $D(\sigma, \sigma')$ be the event when $\sigma$ is nearest to $\sigma'$ among all its permutation.
That is
\begin{equation}
D(\sigma, \sigma') := \{ \sigma = \arg\min_{f \in S_k} \dist(f(\sigma), \sigma')  \}
\end{equation}
Theorem 1 can also be stated in the following way:
\begin{corollary}\label{cor:phase4}
\begin{enumerate}
	\item If $\gamma > b$ and $\beta > \beta^*$, $P_{\SIBM}(\sigma = X | D(\sigma, X))  = 1-o(1)$;
	\item If $\gamma < b$ and $\beta > \beta^*$, $P_{\SIBM}(\sigma = \mathbf{1} | D(\sigma,  \mathbf{1}))  = 1-o(1)$;
	\item If $\gamma > b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma = X | D(\sigma, X))  = o(1)$;
	\item If $\gamma < b$ and $\beta < \beta^*$, $P_{\SIBM}(\sigma = \mathbf{1}| D(\sigma,  \mathbf{1}))  =  o(1)$;
\end{enumerate}	
\end{corollary}
Corollary \ref{cor:phase4} is illustrated in Diagram.

We can generate multiple independent samples from SIBM $\{\sigma^{(1)}, \dots, \sigma^{(m)}\}$, similar to the exact recovery problem of SBM, a natural question related with SIBM is how many samples are sufficient and necessary to recovery $X$?

\section{Degree-corrected Stochastic Ising Block Model}\label{sec:dcsibm}
\section{Parameter Estimation of SBM}\label{sec:psbm}
\section{Community Detection based on Metropolis Sampling}\label{sec:ms}
\section{Conclusion}
\bibliographystyle{IEEEtran}
\bibliography{exportlist}
\end{document}